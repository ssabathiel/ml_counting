{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RL_pick_1s.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"MyIov2HNozeF","colab_type":"code","outputId":"04d16d6b-edf4-469f-964a-6c81b6450863","executionInfo":{"status":"ok","timestamp":1546952372336,"user_tz":-60,"elapsed":569,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}},"colab":{"base_uri":"https://localhost:8080/","height":782}},"cell_type":"code","source":["##############################\n","## CREATE COUNTABLE BINARY ARRAY\n","#############################\n","\n","import numpy as np\n","import scipy.misc as smp\n","import random\n","\n","\n","\n","arr_size_half = 20\n","max_N = 3\n","view_size_half = 0\n","\n","def create_inp_outp_array():\n","    max_objects = 3\n","    n_ones = random.randint(1, max_objects)\n","    one_positions = random.sample(range(view_size_half, view_size_half + max_objects), n_ones)\n","    binary_array = np.zeros(max_objects+2*view_size_half)\n","    binary_array[one_positions] = 255\n","    return binary_array, n_ones\n","    \n","\n"," \n","\n","def make_data_set(n_samples):\n","  mult_inp, mult_out = create_inp_outp_array()\n","  for i in range(n_samples - 1):\n","      single_inp, single_out = create_inp_outp_array()\n","      mult_inp = np.vstack([mult_inp, single_inp])\n","      mult_out = np.vstack((mult_out, single_out))\n","  return mult_inp, mult_out\n","\n","\n","\n","train_input, train_output = make_data_set(1000)\n","max_N=20\n","test_input, test_output = make_data_set(1000)\n","\n","\n","#Print single one\n","inp_ex, out_ex = make_data_set(20)\n","print(\"inp_ex\", inp_ex)\n","print(\"out_ex\", out_ex)\n","\n","\n","#Print set data\n","print(\"===========================\")\n","print(\"train_input: number of examples: \", train_input.shape)\n","print(\"train_input: number of examples: \", train_input[:,0].size)\n","print(\"train_input: single example size: \", train_input[0].size)\n","print(\"train_input: dim of elements in example: \", train_input[0,0].size)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["inp_ex [[255.   0. 255.]\n"," [  0.   0. 255.]\n"," [255.   0. 255.]\n"," [  0.   0. 255.]\n"," [  0.   0. 255.]\n"," [255. 255. 255.]\n"," [  0.   0. 255.]\n"," [  0. 255. 255.]\n"," [255. 255.   0.]\n"," [255. 255.   0.]\n"," [255. 255.   0.]\n"," [255.   0.   0.]\n"," [255. 255. 255.]\n"," [  0. 255. 255.]\n"," [  0.   0. 255.]\n"," [255.   0. 255.]\n"," [  0.   0. 255.]\n"," [255. 255. 255.]\n"," [255. 255. 255.]\n"," [255.   0.   0.]]\n","out_ex [[2]\n"," [1]\n"," [2]\n"," [1]\n"," [1]\n"," [3]\n"," [1]\n"," [2]\n"," [2]\n"," [2]\n"," [2]\n"," [1]\n"," [3]\n"," [2]\n"," [1]\n"," [2]\n"," [1]\n"," [3]\n"," [3]\n"," [1]]\n","===========================\n","train_input: number of examples:  (1000, 3)\n","train_input: number of examples:  1000\n","train_input: single example size:  3\n","train_input: dim of elements in example:  1\n"],"name":"stdout"}]},{"metadata":{"id":"wTQ0uIIiopa_","colab_type":"code","outputId":"0bf2ffab-5d22-4ae3-e672-6c9d7700e078","colab":{"base_uri":"https://localhost:8080/","height":792},"executionInfo":{"status":"error","timestamp":1546952541102,"user_tz":-60,"elapsed":14609,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","#import gym\n","import numpy as np\n","import numpy.random as rd\n","import matplotlib.pyplot as plt\n","#from gym.envs.classic_control.cartpole import CartPoleEnv\n","#from cartpole_utils import plot_results,print_results\n","import tensorflow as tf\n","import copy\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","\n","def init_weights(shape, name):\n","    return tf.Variable(tf.random_normal(shape, stddev=0.01)/ np.sqrt(dim_state), name=name)\n","\n","# This network is the same as the previous one except with an extra hidden layer + dropout\n","def Q_network(state_holder, w_h, w_h2, w_o, b):\n","    h = tf.nn.relu(tf.matmul(state_holder, w_h))\n","    h2 = tf.nn.relu(tf.matmul(h, w_h2))\n","    a_z = tf.matmul(h2, w_o, name='output_activation') + b\n","    Q = tf.nn.sigmoid(a_z, name='Q_model')\n","    return Q\n","\n","\n","########################\n","## SET PARAMETERS\n","########################\n","# Algorithm parameters\n","\n","step_reward = +0\n","final_reward = +1\n","\n","learning_rate = 3e-3\n","gamma = 1.0\n","epsilon = 1.\n","epsi_decay = .995\n","lr_decay = .999\n","epsilon_min = 0.01\n","learning_min = 0.00001\n","\n","\n","# Load Input=Start-of-a-game &&  Classes=Check-for-reward\n","trX = train_input\n","trY = train_output\n","teX = test_input\n","teY = test_output\n","\n","print(trX.shape)\n","print(trY.shape)\n","print(teX.shape)\n","print(teY.shape)\n","\n","## SET ENVIRONMENT + #ofActions #ofStates\n","dim_state = trX[0,:].size\n","n_action = trX[0,:].size\n","\n","\n","\n","# General parameters\n","render = False\n","N_print_every = 100\n","N_trial = trX[:,0].size\n","N_trial_test = N_trial = teX[:,0].size\n","\n","\n","\n","\n","\n","\n","\n","\n","##############################\n","## BUILD NETWORK\n","#############################\n","\n","\n","\n","# PLACEHOLDER\n","action_holder = tf.placeholder(dtype=tf.int32, name=\"Y\")  # +1 because that is the node responsible to say if it is done ore not\n","state_holder = tf.placeholder(dtype=tf.float32, shape=(1, dim_state), name='symbolic_state')\n","next_state_holder = tf.placeholder(dtype=tf.float32, shape=(1, dim_state), name='symbolic_state')\n","\n","# Initialize the parameters of the Q model\n","n_h_neurons = 600\n","w_h = init_weights([trX[0,:].size, n_h_neurons], \"w_h\")\n","w_h2 = init_weights([n_h_neurons, n_h_neurons], \"w_h2\")\n","w_o = init_weights([n_h_neurons, n_action], \"w_o\")\n","b0 = np.zeros(n_action , dtype=np.float32)\n","b = tf.Variable(initial_value=b0, trainable=True, name='bias')\n","\n","Q = Q_network(state_holder, w_h, w_h2, w_o,b)\n","next_Q = Q_network(next_state_holder, w_h, w_h2, w_o,b)\n","\n","\n","\n","### COMPARE REWARDS ####\n","#PLACEHOLDER\n","r_holder = tf.placeholder(dtype=tf.float32, name='symbolic_value_estimation')\n","is_done_holder = tf.placeholder(dtype=tf.float32, name='is_done')\n","\n","next_action_Q_max = tf.to_int32(tf.argmax(next_Q, dimension=1, name='next_step_maxQ_action'))           # [ 0.1   0.4   0.8  0.8] -->  3, 4\n","# next_action_holder = tf.placeholder(dtype=tf.int32, name='symbolic_next_action')\n","R = Q[0, action_holder]\n","next_R = r_holder + gamma * next_Q[0, next_action_Q_max[0]] * (1 - is_done_holder)\n","error = (R - next_R)**2\n","\n","# Define the operation that performs the optimization\n","learning_rate_holder = tf.placeholder(dtype=tf.float32, name='symbolic_state')\n","training_step = tf.train.GradientDescentOptimizer(learning_rate_holder).minimize(error)\n","\n","sess = tf.Session()  # FOR NOW everything is symbolic, this object has to be called to compute each value of Q\n","sess.run(tf.initialize_all_variables())\n","\n","################################\n","################################\n","\n","\n","\n","\n","\n","################################\n","## PI(S)\n","## First check if its random time THEN\n","        # - calculate State--Network--> Q\n","        # - take maximum of Q\n","        # - just remember and return index of maximum    ---> returns ACTION that leads to max_Q-value (not value itself)\n","################################\n","\n","def policy(state):\n","    if rd.rand() < epsilon:\n","        return rd.randint(0, n_action)\n","    reshaped_state = state.reshape(1, dim_state)\n","    Q_values = sess.run(Q, feed_dict={state_holder: state.reshape(1, dim_state)})\n","    Q_values_reduced = Q_values[0, :]\n","    val = np.max(Q_values[0, :])                                                                                      #Change in dimension\n","    max_indices = np.where(Q_values[0, :] == val)[0]\n","    push_finish_button = False\n","    #print(\"Q_values[0,-1]\", Q_values[0,-1])\n","    #if(Q_values[0,-1]>0.0):\n","        #push_finish_button = True\n","\n","    return rd.choice(max_indices), push_finish_button\n","\n","\n","# define Action\n","def draw_point(state, action, finish_button,internal_counter_memory):    #later for convolutoinal NN --> state = matrix rank-2 like a real image is\n","    reward = 0\n","    state_copy = copy.copy(state)\n","    #print(\"draw point\")\n","    #if(finish_button==False):\n","        #print(\"finish_button = False\")\n","    if(state[action]==255):\n","        #print(\"state[action]==1\")\n","        #pass\n","        #print(\"drew on white\")\n","        reward += step_reward                      #Intermediate reward: giving points if set point on white area\n","        internal_counter_memory += 1\n","    state_copy[action] = 0\n","\n","    #print(\"internal_counter_memory\", internal_counter_memory)\n","    return state_copy, reward, internal_counter_memory\n","\n","\n","\n","########################\n","## Get maximum Q_VALUE (not action)\n","#######################\n","def Q_max(state):\n","    Q_values = sess.run(Q, feed_dict={state_holder: state.reshape(1, dim_state)})\n","    return Q_values.max()\n","\n","\n","\n","\n","\n","\n","##############################\n","## ACTUAL RUN\n","###########################\n","\n","time_list = []\n","reward_list = []\n","err_list = []\n","val_list = []\n","mean_reward = 0\n","\n","print(\"Here are the\",  trX[:, 0].size ,\" objects and classes to learn\")\n","for l in range(trX[:, 0].size):\n","    #print(l,\"th object: \", trX[l,:])\n","    #print(l,\"th class : \", trY[l, :])\n","    pass\n","\n","mean_reward_list = []\n","n_episodes = trX[:, 0].size\n","for k in range(n_episodes):\n","\n","    #print(\"N-th episode: \", k)\n","    acc_reward = 0  # Init the accumulated reward\n","    k_n = rd.randint(0, trX[:,0].size)                                            ######   !!!!!!!!!!!!!!!!!\n","\n","    if(epsilon>epsilon_min):\n","        epsilon *= epsi_decay\n","    if (learning_rate > learning_min):\n","        learning_rate *= lr_decay\n","\n","    '''\n","    if k > N_trial+1:\n","        epsilon = 0\n","        learning_rate = 0\n","        observation = teX[k-N_trial, :]\n","        classy = teY[k-N_trial, :]\n","    else:\n","        observation = copy.copy(trX[k, :])\n","        classy = trY[k, :]\n","    '''\n","    observation = copy.copy(trX[k_n, :])\n","    classy = trY[k_n, :]\n","    if(k%50<0):\n","        print(\"Observation:\", observation)\n","\n","    #(\"observation: \", observation)\n","    #print(\"classy: \", classy)\n","    #print(\"-----------------\")\n","\n","    trial_err_list = []\n","    internal_counter_memory = 0\n","    trial_duration = 15\n","    push_finish_button = False\n","\n","    for t in range(trial_duration):  # The number of time steps in this game is maximum 200\n","\n","        #marker, push_finish_button = policy(observation)  # Init the first action\n","        ###########################\n","        if(t==classy):\n","            push_finish_button=True\n","\n","            if (t == classy):\n","                mean_reward += acc_reward\n","                if (k % 50 == 0):\n","                    mean_reward = float(mean_reward) / 50.0\n","                    print(k, \"th mean_reward=\", mean_reward)\n","                    mean_reward_list.append(mean_reward)\n","                    mean_reward = 0.0\n","                break  # Stop the trial when the environment says it is done\n","            '''\n","        if(k>1000 and k<1005 ):\n","            print(\"Before Everything is calculated\")\n","            print(\"t=\", t)\n","            print(\"old_state= \", observation)\n","        '''\n","\n","        old_observation = copy.copy(observation)\n","        old_internal_counter_memory = copy.copy(internal_counter_memory)\n","\n","        Q_values = sess.run(Q, feed_dict={state_holder: observation.reshape(1, dim_state)})\n","        Q_values_reduced = Q_values[0, :]\n","        old_Q_values_reduced = copy.copy(Q_values_reduced)\n","        val = np.max(Q_values_reduced)  # Change in dimension\n","        max_indices = np.where(Q_values_reduced == val)[0]\n","        #push_finish_button = False\n","        #print(\"Q_values[0,-1]\", Q_values[0, -1])\n","        #if (Q_values[0, -1] < -0.5):\n","            #push_finish_button = True\n","\n","        action = rd.choice(max_indices)\n","        if rd.rand() < epsilon:\n","            action = rd.randint(0, n_action)\n","\n","        reward = 0\n","        #print(\"internal_counter_memory before: \", internal_counter_memory)\n","        new_observation, reward, internal_counter_memory = draw_point(observation, action, push_finish_button,internal_counter_memory )\n","        #print(\"internal_counter_memory after: \", internal_counter_memory)\n","\n","\n","        if(t==classy-1):\n","            #Create one-hot-vector from counter\n","            if (internal_counter_memory == classy):\n","                reward += final_reward #10\n","        #print(\"reward: \", reward)\n","\n","        err = sess.run(error, feed_dict={\n","            state_holder: observation.reshape(1, dim_state),\n","            next_state_holder: new_observation.reshape(1, dim_state),\n","            action_holder: action,\n","            is_done_holder: np.float32(push_finish_button),\n","            r_holder: reward})\n","\n","        sess.run(training_step, feed_dict={\n","            state_holder: observation.reshape(1, dim_state),\n","            next_state_holder: new_observation.reshape(1, dim_state),\n","            action_holder: action,\n","            is_done_holder: np.float32(push_finish_button),\n","            r_holder: reward,\n","            learning_rate_holder: learning_rate})\n","\n","        # Compute the Bellman Error for monitoring\n","        err2 = sess.run(error, feed_dict={\n","            state_holder: observation.reshape(1, dim_state),\n","            next_state_holder: new_observation.reshape(1, dim_state),\n","            action_holder: action,\n","            is_done_holder: np.float32(push_finish_button),\n","            r_holder: reward})\n","        trial_err_list.append(err)\n","\n","            #keep track of old observation for print\n","            #if(k>1000 and k<1005 ):\n","             #   print(\"Everything is calculated\")\n","              #  print(\"t=\", t)\n","               # print(\"old_state= \", observation)\n","\n","        Q_values = sess.run(Q, feed_dict={state_holder: old_observation.reshape(1, dim_state)})\n","        Q_values_reduced = Q_values[0, :]\n","\n","        observation = new_observation  # Pass the new state to the next step\n","        acc_reward += reward  # Accumulate the reward\n","        # Add the error in a trial-specific list of errors\n","\n","\n","        weights = sess.run(w_o, feed_dict={state_holder: observation.reshape(1, dim_state)})\n","\n","\n","        #if((k>=0 and k<5) or (k%100==0) ):\n","        if ((k >= 40 and k < 20)):\n","            if(t==0):\n","                print(\"=======================\")\n","                print(\"==== K= \", k, \"========\")\n","            print(\"t=\", t)\n","            print(\"old_state= \", old_observation)\n","            print(\"old_internal_counter_memory\",old_internal_counter_memory)\n","            print(\"Q_values before: \", old_Q_values_reduced)\n","            print(\"action= \", action)\n","            print(\"Q_values after: \", Q_values_reduced)\n","            print(\"state= \", observation)\n","            print(\"internal_counter_memory\", internal_counter_memory)\n","            print(\"acc_reward= \", acc_reward)\n","            print(\"orig nr. of 1s\", classy)\n","            print(\"w_o= \", weights[0,0])\n","            print(\"------End of trial:\")\n","            print(\"Predicted class: \", internal_counter_memory)\n","            print(\"Real class:      \", classy)\n","            print(\"err1 \", err)\n","            print(\"err2 \", err2)\n","            print(\"---------end of live action -------- \\n \\n\")\n","\n","\n","        \n","\n","        '''\n","        if(k%1000==0 and push_finish_button):\n","        #if (k < 50 and push_finish_button):\n","            print(\"k=\",k)\n","            print(\"mean_reward=\", float(mean_reward)/1000.0)\n","            print(\"Last t= \", internal_counter_memory)\n","            print(\"acc_reward\", acc_reward)\n","            print(\"Nth trial\", k)\n","            print(\"State= \", observation)\n","            print(\"Class= \", classy)\n","            print(\"Action= \", action)\n","            #print(\"w_o= \", weights[0, 0])\n","            #print(\"error\",err)\n","            print(\"==============================\")\n","            mean_reward = 0\n","        '''\n","\n","\n","\n","        #internal_counter_memory += 1\n","\n","\n","\n","\n","    # Stack values for monitoring\n","\n","    err_list.append(np.mean(trial_err_list))\n","    time_list.append(t + 1)\n","    reward_list.append(acc_reward)  # Store the result\n","    #print(\"acc_reward\", acc_reward)\n","\n","\n","print(\"mean_reward_list\", mean_reward_list)\n","reward_file = 'Mean_rewards/'  + str(n_h_neurons) + '_neurons_' + str(n_action) + '_max_objects'\n","np.save(reward_file, mean_reward_list)\n","\n","\n","matplotlib.get_backend()\n","#print(acc_arr.shape)\n","#print(episodes.shape)\n","#print(\"np.arange(0,n_episodes)\", np.arange(0,n_episodes))\n","legend_string = str(n_h_neurons) + \" neurons/hidden layer\"\n","plt.plot(np.arange(0,len(mean_reward_list)), mean_reward_list, label = legend_string)\n","plt.legend()\n","print(\"legend_string= \", legend_string)\n","#plt.legend(legend_string,)\n","plt.ylabel('Success rate')\n","plt.xlabel('Episodes')\n","plt.ylim(0,1)\n","title_string = 'Success rate for agent when counting binaries between 0 and ' + str(n_action) + ' binaries'\n","plt.title(title_string)\n","plt.savefig('Q_learning_binaries.png')\n","plt.show()\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["(1000, 3)\n","(1000, 1)\n","(1000, 3)\n","(1000, 1)\n","Here are the 1000  objects and classes to learn\n","0 th mean_reward= 0.0\n","50 th mean_reward= 0.16\n","100 th mean_reward= 0.44\n","150 th mean_reward= 0.58\n","200 th mean_reward= 0.72\n","250 th mean_reward= 0.8\n","300 th mean_reward= 0.82\n","350 th mean_reward= 0.8\n","400 th mean_reward= 0.88\n","450 th mean_reward= 0.88\n","500 th mean_reward= 0.96\n","550 th mean_reward= 0.9\n","600 th mean_reward= 0.94\n","650 th mean_reward= 1.0\n","700 th mean_reward= 0.98\n","750 th mean_reward= 1.0\n","800 th mean_reward= 0.96\n","850 th mean_reward= 1.0\n","900 th mean_reward= 1.0\n","950 th mean_reward= 1.0\n","mean_reward_list [0.0, 0.16, 0.44, 0.58, 0.72, 0.8, 0.82, 0.8, 0.88, 0.88, 0.96, 0.9, 0.94, 1.0, 0.98, 1.0, 0.96, 1.0, 1.0, 1.0]\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-355f03680ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean_reward_list\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0mreward_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Mean_rewards/'\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_h_neurons\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_neurons_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_max_objects'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Mean_rewards/600_neurons_3_max_objects.npy'"]}]},{"metadata":{"id":"CXV5_vrZq8a3","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}