{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AnBn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"s_bAu13u0go2","colab_type":"code","outputId":"58b8fceb-719a-4498-b24e-6200006828d9","executionInfo":{"status":"ok","timestamp":1544106350304,"user_tz":-60,"elapsed":659,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}},"colab":{"base_uri":"https://localhost:8080/","height":153}},"cell_type":"code","source":["########################\n","## Create training data\n","######################\n","\n","import numpy as np\n","import scipy.misc as smp\n","import random\n","\n","#Create input array of the form: [1, 1, 1, 0, 0, 0, 1] (here N=3)\n","                             #     \\ /\\ /\\ /\\ /\\ /\\\n","#Create output array of the form:  [1, 1, 1, 0, 0, 1] \n","\n","\n","arr_size_half = 20\n","max_N = 12\n","\n","\n","def create_inp_outp_array():\n","    #max_N = 8\n","    N = random.randint(1, max_N)\n","    #print(\"N=\", N)\n","    inp_array = np.zeros(2*arr_size_half)\n","    inp_array[0:N] = 1\n","    inp_array[2*N:2*arr_size_half]=1\n","    \n","    out_array = np.zeros(2*arr_size_half)\n","    out_array[0:N] = 1\n","    out_array[2*N-1:2*arr_size_half]=1\n","    \n","    return inp_array, out_array\n"," \n","\n","def make_data_set(n_samples):\n","  mult_inp, mult_out = create_inp_outp_array()\n","  for i in range(n_samples - 1):\n","      single_inp, single_out = create_inp_outp_array()\n","      mult_inp = np.vstack([mult_inp, single_inp])\n","      mult_out = np.vstack((mult_out, single_out))\n","  return mult_inp, mult_out\n","\n","\n","\n","train_input, train_output = make_data_set(1000)\n","max_N=20\n","test_input, test_output = make_data_set(1000)\n","\n","\n","#Print single one\n","inp_ex, out_ex = create_inp_outp_array()\n","print(\"inp_ex\", inp_ex)\n","print(\"out_ex\", out_ex)\n","\n","#Print set data\n","print(\"train_input: number of examples: \", train_input.shape)\n","print(\"train_input: number of examples: \", train_input[:,0].size)\n","print(\"train_input: single example size: \", train_input[0].size)\n","print(\"train_input: dim of elements in example: \", train_input[0,0].size)\n","\n"],"execution_count":66,"outputs":[{"output_type":"stream","text":["inp_ex [1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","out_ex [1. 1. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n","train_input: number of examples:  (1000, 40)\n","train_input: number of examples:  1000\n","train_input: single example size:  40\n","train_input: dim of elements in example:  1\n"],"name":"stdout"}]},{"metadata":{"id":"Od4lIsChsU_p","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":3179},"outputId":"0b9c29b6-8579-4b54-a4cb-705093d86736","executionInfo":{"status":"ok","timestamp":1544107077288,"user_tz":-60,"elapsed":21862,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}}},"cell_type":"code","source":["##############################\n","## A^N B^N\n","##################################\n","\n","import numpy as np\n","import scipy.misc as smp\n","import random\n","#Source code with the blog post at http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/\n","import numpy as np\n","import random\n","from random import shuffle\n","import tensorflow as tf\n","tf.reset_default_graph()\n","\n","# from tensorflow.models.rnn import rnn_cell\n","# from tensorflow.models.rnn import rnn\n","\n","NUM_EXAMPLES = 10000\n","\n","# Create input data\n","\n","print(\"test and training data loaded\")\n","\n","\n","inp_size = train_input[0].size\n","out_size = train_output[0].size\n","\n","data = tf.placeholder(tf.float32, [None, inp_size,1]) #Number of examples, number of input, dimension of each input                # change shape: get first array length from input_data and output_data\n","target = tf.placeholder(tf.float32, [None, out_size,1])                                                                              # change shape: get first array length from input_data and output_data\n","num_hidden = 1                                                                                                              # maybe change this?\n","#cell = tf.contrib.rnn.LSTMCell(num_hidden,state_is_tuple=True)\n","cell = tf.nn.rnn_cell.GRUCell(num_hidden)\n","val, _ = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\n","val = tf.transpose(val, [1, 0, 2])\n","#last = tf.gather(val, int(val.get_shape()[0]) - 1)\n","last = val #--> not only take last.. do it in eager execution to check everything\n","############new\n","weight = tf.Variable(tf.truncated_normal([num_hidden, 1]))                                           #--> num_hidden, 1] \n","bias = tf.Variable(tf.constant(0.1, shape=[1]))\n","\n","\n","#Get output of all steps\n","#transformed_outputs = [tf.matmul(val_i, weight + bias) for val_i in val]\n","transformed_outputs = tf.map_fn(lambda val_i: tf.matmul(val_i, weight + bias), val)\n","\n","\n","'''\n","i = tf.constant(0)\n","while_condition = lambda i: tf.less(i, train_input[0].size)\n","def body(i):\n","    # do something here which you want to do in your loop\n","    # increment i\n","    tf.matmul(val[i], weight + bias)\n","    return [tf.add(i, 1)]\n","# do the loop:\n","r = tf.while_loop(while_condition, body, [i])\n","'''\n","\n","transformed_outputs = tf.transpose(transformed_outputs, [1, 0, 2])\n","binary_single_transformed_outputs =  tf.nn.sigmoid(transformed_outputs)\n","round_binary_single_transformed_outputs = tf.round(binary_single_transformed_outputs)\n","cross_entropy = tf.reduce_sum( (target - binary_single_transformed_outputs)*(target - binary_single_transformed_outputs) )\n","\n","\n","\n","\n","###########\n","optimizer = tf.train.AdamOptimizer()\n","minimize = optimizer.minimize(cross_entropy)\n","mistakes = tf.not_equal(target,  tf.round(binary_single_transformed_outputs))\n","error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n","\n","init_op = tf.initialize_all_variables()\n","sess = tf.Session()\n","sess.run(init_op)\n","\n","batch_size = 40\n","no_of_batches = int(int(len(train_input)) / batch_size)\n","epoch = 70\n","for i in range(epoch):\n","    ptr = 0\n","    for j in range(no_of_batches):\n","        inp, out = train_input[ptr:ptr+batch_size], train_output[ptr:ptr+batch_size]\n","        ptr+=batch_size\n","        sess.run(minimize,{data: inp.reshape(batch_size, inp_size,1), target: out.reshape(batch_size, out_size,1)})\n","    print(\"Epoch \",str(i) )\n","    incorrect = sess.run(error,{data: test_input.reshape(test_input[:,0].size, inp_size,1), target: test_output.reshape(test_output[:,0].size, inp_size,1)})\n","    print('Epoch {:2d} error {:3.1f}%'.format(i + 1, 100 * incorrect))\n","\n","#create single-test example\n","inp_ex, out_ex = create_inp_outp_array()\n","print(\"Test input: \", inp_ex)\n","print(\"Should output: \", out_ex)\n","print(\"Predicted output: \")\n","print(sess.run(round_binary_single_transformed_outputs,{data: inp_ex.reshape(1, inp_size,1)}))\n","sess.close()\n"," \n","\n","\n"],"execution_count":70,"outputs":[{"output_type":"stream","text":["test and training data loaded\n","Epoch  0\n","Epoch  1 error 23.5%\n","Epoch  1\n","Epoch  2 error 23.5%\n","Epoch  2\n","Epoch  3 error 23.5%\n","Epoch  3\n","Epoch  4 error 23.5%\n","Epoch  4\n","Epoch  5 error 23.5%\n","Epoch  5\n","Epoch  6 error 23.5%\n","Epoch  6\n","Epoch  7 error 23.5%\n","Epoch  7\n","Epoch  8 error 23.5%\n","Epoch  8\n","Epoch  9 error 23.5%\n","Epoch  9\n","Epoch 10 error 23.5%\n","Epoch  10\n","Epoch 11 error 23.5%\n","Epoch  11\n","Epoch 12 error 23.5%\n","Epoch  12\n","Epoch 13 error 23.5%\n","Epoch  13\n","Epoch 14 error 23.5%\n","Epoch  14\n","Epoch 15 error 23.5%\n","Epoch  15\n","Epoch 16 error 23.5%\n","Epoch  16\n","Epoch 17 error 23.5%\n","Epoch  17\n","Epoch 18 error 23.5%\n","Epoch  18\n","Epoch 19 error 23.5%\n","Epoch  19\n","Epoch 20 error 23.5%\n","Epoch  20\n","Epoch 21 error 23.5%\n","Epoch  21\n","Epoch 22 error 23.5%\n","Epoch  22\n","Epoch 23 error 23.5%\n","Epoch  23\n","Epoch 24 error 23.5%\n","Epoch  24\n","Epoch 25 error 23.5%\n","Epoch  25\n","Epoch 26 error 23.5%\n","Epoch  26\n","Epoch 27 error 19.4%\n","Epoch  27\n","Epoch 28 error 15.6%\n","Epoch  28\n","Epoch 29 error 12.6%\n","Epoch  29\n","Epoch 30 error 10.8%\n","Epoch  30\n","Epoch 31 error 10.8%\n","Epoch  31\n","Epoch 32 error 8.9%\n","Epoch  32\n","Epoch 33 error 8.9%\n","Epoch  33\n","Epoch 34 error 6.9%\n","Epoch  34\n","Epoch 35 error 6.9%\n","Epoch  35\n","Epoch 36 error 6.9%\n","Epoch  36\n","Epoch 37 error 7.0%\n","Epoch  37\n","Epoch 38 error 6.9%\n","Epoch  38\n","Epoch 39 error 4.8%\n","Epoch  39\n","Epoch 40 error 4.8%\n","Epoch  40\n","Epoch 41 error 4.8%\n","Epoch  41\n","Epoch 42 error 4.8%\n","Epoch  42\n","Epoch 43 error 4.8%\n","Epoch  43\n","Epoch 44 error 4.8%\n","Epoch  44\n","Epoch 45 error 4.9%\n","Epoch  45\n","Epoch 46 error 4.9%\n","Epoch  46\n","Epoch 47 error 4.9%\n","Epoch  47\n","Epoch 48 error 4.9%\n","Epoch  48\n","Epoch 49 error 4.8%\n","Epoch  49\n","Epoch 50 error 2.5%\n","Epoch  50\n","Epoch 51 error 2.5%\n","Epoch  51\n","Epoch 52 error 2.5%\n","Epoch  52\n","Epoch 53 error 2.5%\n","Epoch  53\n","Epoch 54 error 2.5%\n","Epoch  54\n","Epoch 55 error 2.5%\n","Epoch  55\n","Epoch 56 error 2.5%\n","Epoch  56\n","Epoch 57 error 2.5%\n","Epoch  57\n","Epoch 58 error 2.5%\n","Epoch  58\n","Epoch 59 error 2.5%\n","Epoch  59\n","Epoch 60 error 2.5%\n","Epoch  60\n","Epoch 61 error 2.5%\n","Epoch  61\n","Epoch 62 error 2.5%\n","Epoch  62\n","Epoch 63 error 2.5%\n","Epoch  63\n","Epoch 64 error 2.5%\n","Epoch  64\n","Epoch 65 error 2.5%\n","Epoch  65\n","Epoch 66 error 2.5%\n","Epoch  66\n","Epoch 67 error 2.5%\n","Epoch  67\n","Epoch 68 error 2.5%\n","Epoch  68\n","Epoch 69 error 2.5%\n","Epoch  69\n","Epoch 70 error 2.5%\n","Test input:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1.]\n","Should output:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.]\n","Predicted output: \n","[[[1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [0.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]\n","  [1.]]]\n"],"name":"stdout"}]},{"metadata":{"id":"Z_VovgJpGI0O","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1421},"outputId":"60a6cda9-7392-478e-ea79-dd410f46a720","executionInfo":{"status":"error","timestamp":1544097412679,"user_tz":-60,"elapsed":566,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}}},"cell_type":"code","source":["##############################\n","## A^N B^N  - EAGER EXECUTION  for debugging\n","##################################\n","\n","import numpy as np\n","import scipy.misc as smp\n","import random\n","#Source code with the blog post at http://monik.in/a-noobs-guide-to-implementing-rnn-lstm-using-tensorflow/\n","import numpy as np\n","import random\n","from random import shuffle\n","import tensorflow as tf\n","tf.reset_default_graph()\n","tf.enable_eager_execution()\n","\n","# from tensorflow.models.rnn import rnn_cell\n","# from tensorflow.models.rnn import rnn\n","\n","NUM_EXAMPLES = 10000\n","\n","# Create input data\n","\n","print(\"test and training data loaded\")\n","\n","\n","inp_size = train_input[0].size\n","out_size = train_output[0].size\n","\n","\n","\n","#data = tf.placeholder(tf.float32, [None, inp_size,1]) #Number of examples, number of input, dimension of each input                # change shape: get first array length from input_data and output_data\n","#target = tf.placeholder(tf.float32, [None, out_size])                                                                              # change shape: get first array length from input_data and output_data\n","batch_size = 1000\n","ptr = 0\n","data = tf.convert_to_tensor(train_input[ptr:ptr+batch_size].reshape(batch_size, inp_size,1), dtype=tf.float32 )\n","target = tf.convert_to_tensor(train_output[ptr:ptr+batch_size].reshape(batch_size, out_size,1), dtype=tf.float32 )\n","num_hidden = 24                                                                                                              # maybe change this?\n","cell = tf.contrib.rnn.LSTMCell(num_hidden,state_is_tuple=True)\n","val, _ = tf.nn.dynamic_rnn(cell, data, dtype=tf.float32)\n","val = tf.transpose(val, [1, 0, 2])\n","\n","\n","\n","\n","print(\"val.shape: \", tf.shape(val))\n","last = tf.gather(val, int(val.get_shape()[0]) - 1)\n","print(\"last.shape: \", tf.shape(last))\n","last = val #--> not only take last.. do it in eager execution to check everything\n","weight = tf.Variable(tf.truncated_normal([num_hidden, 1]))                                           #--> num_hidden, 1] \n","bias = tf.Variable(tf.constant(0.1, shape=[1]))\n","\n","\n","#Get output of all steps\n","transformed_outputs = [tf.matmul(val_i, weight + bias) for val_i in val]\n","print(\"transformed_outputs.shape: \", tf.shape(transformed_outputs))\n","transformed_outputs = tf.transpose(transformed_outputs, [1, 0, 2])\n","print(\"transformed_outputs.shape: \", tf.shape(transformed_outputs))\n","\n","single_transformed_outputs = transformed_outputs #tf.gather(transformed_outputs, 0)\n","print(\"single_transformed_outputs.shape: \", tf.shape(single_transformed_outputs))\n","binary_single_transformed_outputs =  tf.round(tf.nn.sigmoid(single_transformed_outputs))\n","print(\"binary_single_transformed_outputs: \", binary_single_transformed_outputs)\n","\n","\n","#prediction = tf.nn.softmax(tf.matmul(last, weight) + bias)\n","cross_entropy = -tf.reduce_sum(target * binary_single_transformed_outputs)\n","optimizer = tf.train.AdamOptimizer()\n","minimize = optimizer.minimize(cross_entropy)\n","mistakes = tf.not_equal(tf.argmax(target, 1), tf.argmax(prediction, 1))\n","error = tf.reduce_mean(tf.cast(mistakes, tf.float32))\n","\n","init_op = tf.initialize_all_variables()\n","sess = tf.Session()\n","sess.run(init_op)\n","\n","batch_size = 1000\n","no_of_batches = int(int(len(train_input)) / batch_size)\n","epoch = 20\n","for i in range(epoch):\n","    ptr = 0\n","    for j in range(no_of_batches):\n","        inp, out = train_input[ptr:ptr+batch_size], train_output[ptr:ptr+batch_size]\n","        ptr+=batch_size\n","        sess.run(minimize,{data: inp, target: out})\n","    print(\"Epoch \",str(i) )\n","incorrect = sess.run(error,{data: test_input, target: test_output})\n","print('Epoch {:2d} error {:3.1f}%'.format(i + 1, 100 * incorrect))\n","print(np.argmax(sess.run(prediction,{data: [[[1],[0],[0],[1],[1],[0],[1],[1],[1],[0],[1],[0],[0],[1],[1],[0],[1],[1],[1],[0]]]})) )           #Change this to appropriate input\n","sess.close()\n"," \n","\n","\n"],"execution_count":24,"outputs":[{"output_type":"stream","text":["test and training data loaded\n","val.shape:  tf.Tensor([   8 1000   24], shape=(3,), dtype=int32)\n","last.shape:  tf.Tensor([1000   24], shape=(2,), dtype=int32)\n","transformed_outputs.shape:  tf.Tensor([   8 1000    1], shape=(3,), dtype=int32)\n","transformed_outputs.shape:  tf.Tensor([1000    8    1], shape=(3,), dtype=int32)\n","single_transformed_outputs.shape:  tf.Tensor([1000    8    1], shape=(3,), dtype=int32)\n","binary_single_transformed_outputs:  tf.Tensor(\n","[[[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," ...\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]\n","\n"," [[0.]\n","  [0.]\n","  [0.]\n","  ...\n","  [0.]\n","  [0.]\n","  [0.]]], shape=(1000, 8, 1), dtype=float32)\n"],"name":"stdout"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-d10c52fc85a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mcross_entropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbinary_single_transformed_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mminimize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mmistakes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmistakes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m       raise RuntimeError(\n\u001b[0;32m--> 483\u001b[0;31m           \u001b[0;34m\"`loss` passed to Optimizer.compute_gradients should \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m           \"be a function when eager execution is enabled.\")\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: `loss` passed to Optimizer.compute_gradients should be a function when eager execution is enabled."]}]},{"metadata":{"id":"VMnNdyxUHOq0","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}