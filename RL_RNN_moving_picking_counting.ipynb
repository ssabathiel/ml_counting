{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy4 of RL_from_NN_to_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "O41vLsJc6zqs",
        "colab_type": "code",
        "outputId": "3f30d5b9-5083-464c-cef1-f6243231def9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## CREATE COUNTABLE BINARY ARRAY with field view !!!!!!\n",
        "#############################\n",
        "\n",
        "import numpy as np\n",
        "import scipy.misc as smp\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "arr_size_half = 20\n",
        "max_N = 1\n",
        "view_size_half = 3\n",
        "object_value = 100.\n",
        "\n",
        "def create_inp_outp_array():\n",
        "    max_objects = 4\n",
        "    n_ones = random.randint(1, max_objects)\n",
        "    one_positions = random.sample(range(view_size_half, view_size_half + max_objects), n_ones)\n",
        "    binary_array = np.zeros(max_objects+2*view_size_half)\n",
        "    binary_array[one_positions] = object_value\n",
        "    #binary_array[2] = object_value\n",
        "    binary_array[0] = 9\n",
        "    binary_array[-1] = 9\n",
        "\n",
        "    return binary_array, n_ones\n",
        "    \n",
        "\n",
        " \n",
        "\n",
        "def make_data_set(n_samples):\n",
        "  mult_inp, mult_out = create_inp_outp_array()\n",
        "  for i in range(n_samples - 1):\n",
        "      single_inp, single_out = create_inp_outp_array()\n",
        "      mult_inp = np.vstack([mult_inp, single_inp])\n",
        "      mult_out = np.vstack((mult_out, single_out))\n",
        "  return mult_inp, mult_out\n",
        "\n",
        "\n",
        "\n",
        "train_input, train_output = make_data_set(10000)\n",
        "max_N=20\n",
        "test_input, test_output = make_data_set(10000)\n",
        "\n",
        "\n",
        "#Print single one\n",
        "inp_ex, out_ex = make_data_set(10)\n",
        "print(\"inp_ex\", inp_ex)\n",
        "print(\"out_ex\", out_ex)\n",
        "\n",
        "\n",
        "#Print set data\n",
        "print(\"===========================\")\n",
        "print(\"train_input: number of examples: \", train_input.shape)\n",
        "print(\"train_input: number of examples: \", train_input[:,0].size)\n",
        "print(\"train_input: single example size: \", train_input[0].size)\n",
        "print(\"train_input: dim of elements in example: \", train_input[0,0].size)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inp_ex [[  9.   0.   0.   0. 100. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0.   0.   0. 100.   0.   0.   0.   9.]\n",
            " [  9.   0.   0.   0.   0. 100.   0.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100.   0. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100.   0. 100.   0.   0.   9.]\n",
            " [  9.   0.   0.   0. 100. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100.   0. 100.   0.   0.   0.   9.]\n",
            " [  9.   0.   0.   0.   0.   0. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100. 100. 100.   0.   0.   9.]]\n",
            "out_ex [[3]\n",
            " [1]\n",
            " [1]\n",
            " [3]\n",
            " [4]\n",
            " [3]\n",
            " [3]\n",
            " [2]\n",
            " [1]\n",
            " [4]]\n",
            "===========================\n",
            "train_input: number of examples:  (10000, 10)\n",
            "train_input: number of examples:  10000\n",
            "train_input: single example size:  10\n",
            "train_input: dim of elements in example:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bp51BbMs8GYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "## UPDATE ENVIRONMENT WHEN MOVING, PICKING WITH LIMITED FIELD VIEW\n",
        "##################################################################\n",
        "\n",
        "\n",
        "def update_env(state, pos, action, reward,t):\n",
        "  \n",
        "  push_finish_button = False\n",
        "  reward=0.0\n",
        "  if(action==0):                      #[0] ----> pick the one at the current position\n",
        "    if(state[pos]==object_value):\n",
        "      state[pos]= 0\n",
        "      reward += point_reward\n",
        "    if(state[pos]==0):\n",
        "      reward += wrong_point_reward     \n",
        "  elif(action==1):                    #[1] ----> stop the current counting process\n",
        "    push_finish_button = True\n",
        "  elif(action==2):                    #[2] ----> go to the left\n",
        "    if(pos>view_size_half):\n",
        "      pos-=1\n",
        "  elif(action==3):                    #[3] ----> go to the right\n",
        "     if(pos<state.size - view_size_half-1):\n",
        "      pos+=1\n",
        "  else:\n",
        "    (\"How could your action go beyond N+3?\")\n",
        "    \n",
        "  if(push_finish_button==False):\n",
        "    reward += time_penalty_reward\n",
        "  if (push_finish_button==True or t==trial_duration-1):\n",
        "    if(state[1]==0 and state[2]==0):\n",
        "      reward += final_reward\n",
        "  if(state[1]==0 and state[2]==0):\n",
        "      reward += final_reward\n",
        "      push_finish_button = True\n",
        "    \n",
        "  return state, pos, push_finish_button, reward \n",
        "    \n",
        "    \n",
        "def get_field_view(state, pos, view_size_half):\n",
        "  fiew_field =  copy.copy(state[pos-view_size_half:pos+view_size_half+1])\n",
        "  return fiew_field\n",
        "\n",
        "def action_int_to_letter(action):\n",
        "  \n",
        "  if(action==0):          #[0] ----> pick the one at the current position\n",
        "     act_str = \"P\"\n",
        "  elif(action==1):        #[1] ----> stop the current counting proces\n",
        "     act_str = \"STOP\"\n",
        "  elif(action==2):        #[2] ----> go to the left\n",
        "    act_str = \"L\"\n",
        "  elif(action==3):        #[3] ----> go to the right\n",
        "    act_str = \"R\"\n",
        "  else:\n",
        "    (\"How could your action go beyond N+3?\")\n",
        "    \n",
        "  return act_str\n",
        "    \n",
        "\n",
        "def get_animation_pic(state,pos,view_size_half,action):\n",
        "  s = \" \"\n",
        "  for i in range(pos-view_size_half):\n",
        "    s+=\"   \"\n",
        "  s += \"|\"\n",
        "  for i in range(view_size_half):\n",
        "    s+=\"__\"\n",
        "  s += \"I\"\n",
        "  for i in range(view_size_half):\n",
        "    s+=\"__\"\n",
        "  s += \"|\"\n",
        "  for i in range(state.size - (pos-view_size_half+3) ):\n",
        "    s+=\"   \"\n",
        "  s+= \"  \"\n",
        "  act_str = action_int_to_letter(action)\n",
        "  s+= act_str\n",
        "  print(state)\n",
        "  print(s)\n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "class ExperienceBuffer:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.observation = np.array([]).reshape(0,dim_obs)\n",
        "    self.new_observation = np.array([]).reshape(0,dim_obs)\n",
        "    self.action = np.empty(0)\n",
        "    self.push_finish_button = np.empty(0)\n",
        "    self.reward = np.empty(0)\n",
        "  \n",
        "  \n",
        "  def add_exp(self, observation, new_observation, action, reward, push_finish_button):\n",
        "  \n",
        "    if(len(exp_buffer.observation) < batch_size):  \n",
        "\n",
        "        self.observation = np.vstack([exp_buffer.observation, observation])\n",
        "        self.new_observation = np.vstack([exp_buffer.new_observation, new_observation])\n",
        "        self.action = np.append(exp_buffer.action, action)\n",
        "        self.reward = np.append(exp_buffer.reward, reward)\n",
        "        self.push_finish_button = np.append(exp_buffer.push_finish_button, np.float32(push_finish_button))\n",
        "\n",
        "    else:\n",
        "        rand_i = random.randint(0,batch_size-1)\n",
        "        self.observation[rand_i] = observation\n",
        "        self.new_observation[rand_i] = new_observation\n",
        "        self.action[rand_i] = action\n",
        "        self.reward[rand_i] = reward\n",
        "        self.push_finish_button[rand_i] = np.float32(push_finish_button)\n",
        "        \n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tO5pXpE5frNJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################\n",
        "## SET PARAMETERS\n",
        "########################\n",
        "# RL Parameters\n",
        "\n",
        "point_reward = +0.1\n",
        "wrong_point_reward = -0.0\n",
        "time_penalty_reward = -0.05\n",
        "final_reward = +1.0\n",
        "stop_reward = +0.0\n",
        "trial_duration = 10\n",
        "\n",
        "\n",
        "# NETWORK LEARNING PARAMETERS\n",
        "batch_size = 1\n",
        "buffer_size = 1\n",
        "gamma = 1.0\n",
        "epsilon_init = 1.\n",
        "learning_rate_init = 3e-4\n",
        "epsi_decay = .999\n",
        "lr_decay = .999\n",
        "epsilon_min = 0.01\n",
        "learning_min = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NfZJIp7sEtOy",
        "colab_type": "code",
        "outputId": "ccfad5d8-6f98-4c18-d943-ba6d40a915b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5926
        }
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "#import gym\n",
        "import numpy as np\n",
        "import numpy.random as rd\n",
        "import matplotlib.pyplot as plt\n",
        "#from gym.envs.classic_control.cartpole import CartPoleEnv\n",
        "#from cartpole_utils import plot_results,print_results\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def init_weights(shape, name):\n",
        "    return tf.Variable(tf.random_normal(shape, stddev=0.01)/ np.sqrt(dim_obs), name=name)\n",
        "\n",
        "\n",
        "def Q_network(state_holder, w_h, w_h2, w_o, b, b_h):\n",
        "    h = tf.nn.relu(tf.matmul(state_holder, w_h) + b_h )\n",
        "    #h2 = tf.nn.relu(tf.matmul(h, w_h2))\n",
        "    Q =  tf.matmul(h, w_o, name='output_activation') + b \n",
        "    #Q = tf.nn.sigmoid(a_z, name='Q_model')\n",
        "    return Q\n",
        "  \n",
        "def Q_RNN_Network(cell, obs_holder, net_state_sess, weight, bias):\n",
        "  val, next_net_state = tf.nn.dynamic_rnn(cell, obs_holder, initial_state=net_state_sess, dtype=tf.float32)\n",
        "  val = tf.transpose(val, [1, 0, 2])\n",
        "  last = tf.gather(val, int(val.get_shape()[0]) - 1)\n",
        "  Q_i = tf.matmul(last, weight) + bias\n",
        "  return Q_i, next_net_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load Input=Start-of-a-game &&  Classes=Check-for-reward\n",
        "trX = train_input\n",
        "trY = train_output\n",
        "teX = test_input\n",
        "teY = test_output\n",
        "\n",
        "\n",
        "\n",
        "## SET ENVIRONMENT + #ofActions #ofStates\n",
        "n_action = 4     #[Pick, Stop, Left, Right]\n",
        "dim_obs = 1+2*view_size_half\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## BUILD TENSORFLOW\n",
        "#############################\n",
        "\n",
        "\n",
        "# PLACEHOLDER\n",
        "action_holder = tf.placeholder(dtype=tf.int32, shape=(None, 1), name=\"action_holder\")  # +1 because that is the node responsible to say if it is done ore not\n",
        "state_holder = tf.placeholder(dtype=tf.float32, shape=(None, dim_obs), name='state_holder')\n",
        "next_state_holder = tf.placeholder(dtype=tf.float32, shape=(None, dim_obs), name='next_state_holder')\n",
        "\n",
        "\n",
        "#PLACEHOLDER\n",
        "r_holder = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='r_holder')\n",
        "is_done_holder = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='is_done_holder')\n",
        "learning_rate_holder = tf.placeholder(dtype=tf.float32, name='symbolic_state')\n",
        "\n",
        "\n",
        "# Q - NETWORK\n",
        "n_h_neurons = 20\n",
        "w_h = init_weights([dim_obs, n_h_neurons], \"w_h\")\n",
        "w_h2 = init_weights([n_h_neurons, n_h_neurons], \"w_h2\")\n",
        "w_o = init_weights([n_h_neurons, n_action], \"w_o\")\n",
        "b0 = np.zeros(n_action, dtype=np.float32)\n",
        "b_h0 = np.zeros(n_h_neurons, dtype=np.float32)\n",
        "b = tf.Variable(initial_value=b0, trainable=True, name='bias')\n",
        "b_h = tf.Variable(initial_value=b_h0, trainable=True, name='bias')\n",
        "\n",
        "\n",
        "# Q - RNN NETWORK\n",
        "# Initialize the parameters of the Q-RNN model\n",
        "num_hidden = 20\n",
        "weight = tf.Variable(tf.truncated_normal([num_hidden, n_action]))\n",
        "bias = tf.Variable(tf.constant(0.1, shape=[n_action]))\n",
        "\n",
        "cell = tf.contrib.rnn.LSTMCell(num_hidden,state_is_tuple=True,reuse=tf.AUTO_REUSE)\n",
        "\n",
        "net_state_sess = cell.zero_state(batch_size, tf.float32)\n",
        "next_net_state = cell.zero_state(batch_size, tf.float32)\n",
        "\n",
        "\n",
        "#############################\n",
        "\n",
        "\n",
        "###################\n",
        "### CALCULATE Q\n",
        "################\n",
        "Q = Q_network(state_holder, w_h, w_h2, w_o,b, b_h)\n",
        "next_Q = Q_network(next_state_holder, w_h, w_h2, w_o,b, b_h)\n",
        "#Q, next_net_state = Q_RNN_Network(cell, obs_holder, net_state_sess, weight, bias)\n",
        "#next_Q = Q_RNN_Network(cell, next_obs_holder, net_state_sess, weight, bias)\n",
        "\n",
        "\n",
        "\n",
        "### COMPARE REWARD<->PREDICTED REWARD ===> LOSS ####\n",
        "\n",
        "#### From https://lilianweng.github.io/lil-log/2018/05/05/implementing-deep-reinforcement-learning-models.html\n",
        "# The prediction by the primary Q network for the actual actions.\n",
        "action_one_hot = tf.one_hot(action_holder, n_action, 1.0, 0.0, name='action_one_hot')\n",
        "R = tf.reduce_sum(Q * action_one_hot, reduction_indices=-1, name='q_acted')\n",
        "\n",
        "# The optimization target defined by the Bellman equation and the target network.\n",
        "max_q_next_by_target = tf.reduce_max(next_Q, axis=-1)\n",
        "R_next = r_holder + (1. - is_done_holder) * gamma * max_q_next_by_target\n",
        "\n",
        "# The loss measures the mean squared error between prediction and target.\n",
        "err = tf.reduce_mean(tf.square(R - tf.stop_gradient(R_next)), name=\"loss_mse_train\")\n",
        "training_step = tf.train.AdamOptimizer(learning_rate=learning_rate_holder).minimize(err, name=\"adam_optim\")\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sess = tf.Session()  # FOR NOW everything is symbolic, this object has to be called to compute each value of Q\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "################################\n",
        "################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################\n",
        "## PI(S)\n",
        "## First check if its random time THEN\n",
        "        # - calculate State--Network--> Q\n",
        "        # - take maximum of Q\n",
        "        # - just remember and return index of maximum    ---> returns ACTION that leads to max_Q-value (not value itself)\n",
        "################################\n",
        "\n",
        "def policy(observation, net_state):\n",
        "    #Q_values = sess.run(Q, feed_dict={state_holder: observation.reshape(1, dim_obs)} )\n",
        "    Q_values, net_state = sess.run((Q, next_net_state), feed_dict={state_holder: observation.reshape(1, dim_obs), net_state_sess: net_state})\n",
        "\n",
        "    Q_values_reduced = Q_values[0, :]\n",
        "    val = np.max(Q_values_reduced)  # Change in dimension\n",
        "    max_indices = np.where(Q_values_reduced == val)[0]\n",
        "    \n",
        "    action = 0\n",
        "    if rd.rand() < epsilon:\n",
        "      action = rd.randint(0, n_action)\n",
        "    else:\n",
        "      action = rd.choice(max_indices)\n",
        "\n",
        "    return action, net_state \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## ACTUAL RUN\n",
        "###########################\n",
        "\n",
        "time_list = []\n",
        "reward_list = []\n",
        "err_list = []\n",
        "val_list = []\n",
        "trial_err_list = []\n",
        "mean_reward_list = []\n",
        "\n",
        "mean_reward = 0\n",
        "print_every_n = 50\n",
        "epsilon = epsilon_init\n",
        "learning_rate = learning_rate_init\n",
        "n_episodes = trX[:, 0].size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "\n",
        "for k in range(n_episodes):\n",
        "\n",
        "    acc_reward = 0                  # Init the accumulated reward\n",
        "    push_finish_button = False\n",
        "    internal_counter_memory = 0\n",
        "    \n",
        "    # Initialize external and internal state of agent\n",
        "    k_n = rd.randint(0, trX[:,0].size)                                            ######   !!!!!!!!!!!!!!!!!\n",
        "    state = copy.copy(trX[k_n, :])\n",
        "    pos = view_size_half\n",
        "    observation = get_field_view(state, pos, view_size_half)\n",
        "    net_state = sess.run(net_state_sess)\n",
        "        \n",
        "    \n",
        "    for t in range(trial_duration):  \n",
        "        \n",
        "\n",
        "        old_observation = copy.copy(observation)\n",
        "        old_internal_counter_memory = copy.copy(internal_counter_memory)\n",
        "        state_copy = copy.copy(state)\n",
        "\n",
        "        #####################\n",
        "        ## Chose action by Random or Maximum Q-value\n",
        "        ################################\n",
        "          \n",
        "        action, net_state = policy(observation,net_state)\n",
        "          \n",
        "        ######################\n",
        "        ## Update Environment, get reward according to chosen action\n",
        "        #################################################    \n",
        "        new_state, pos, push_finish_button, reward = update_env(state_copy, pos, action, reward, t)\n",
        "        new_observation = get_field_view(new_state, pos, view_size_half)\n",
        "        exp_buffer.add_exp(observation, new_observation, action, reward, push_finish_button)\n",
        "\n",
        "\n",
        "        \n",
        "        ############################\n",
        "        ## TRAINING STEP, ERROR CALCULATION (RUN SESSION)\n",
        "        ############################\n",
        "                \n",
        "        \n",
        "        buffer_size = exp_buffer.observation[:,0].size\n",
        "        \n",
        "        if(buffer_size >= batch_size):\n",
        "     \n",
        "          error = sess.run(err, feed_dict={\n",
        "              state_holder: exp_buffer.observation.reshape(batch_size, dim_obs),\n",
        "              next_state_holder: exp_buffer.new_observation.reshape(batch_size, dim_obs) ,\n",
        "              net_state_sess: net_state,\n",
        "              action_holder: exp_buffer.action.reshape(batch_size, 1),\n",
        "              is_done_holder: exp_buffer.push_finish_button.reshape(batch_size, 1),\n",
        "              r_holder: exp_buffer.reward.reshape(batch_size, 1)})   \n",
        "\n",
        "          sess.run(training_step, feed_dict={\n",
        "              state_holder: exp_buffer.observation.reshape(batch_size, dim_obs),\n",
        "              next_state_holder: exp_buffer.new_observation.reshape(batch_size, dim_obs),\n",
        "              net_state_sess: net_state,\n",
        "              action_holder: exp_buffer.action.reshape(batch_size, 1),\n",
        "              is_done_holder: exp_buffer.push_finish_button.reshape(batch_size, 1),\n",
        "              r_holder: exp_buffer.reward.reshape(batch_size, 1),\n",
        "              learning_rate_holder: learning_rate})\n",
        "        \n",
        "\n",
        "          if(epsilon>epsilon_min):\n",
        "             epsilon *= epsi_decay\n",
        "          if (learning_rate > learning_min):\n",
        "             learning_rate *= lr_decay\n",
        "        \n",
        "\n",
        "        observation = new_observation  # Pass the new state to the next step\n",
        "        old_state = copy.copy(state)\n",
        "        state = new_state\n",
        "        acc_reward += reward  # Accumulate the reward\n",
        "        #trial_err_list.append(error)\n",
        "\n",
        "        \n",
        "        ########################\n",
        "        ## PRINT IN SINGLE TIME STEPS\n",
        "        #########################\n",
        "                    \n",
        "        if( (k>=0 and k<9) and (k>=700 and k<709) ):\n",
        "          if(t==0):\n",
        "            print(\"\\n ================\")\n",
        "            print(\"K= \", k)\n",
        "          print(\"t: \", t)\n",
        "          print(\"Action\", action)\n",
        "          get_animation_pic(state,pos,view_size_half,action)\n",
        "          print(\"observation 2: \", observation)      \n",
        "          print(\"Reward: \", reward)\n",
        "          print(\"--------------\")\n",
        "         \n",
        "        if (push_finish_button==True or t==trial_duration-1):\n",
        "          mean_reward += acc_reward\n",
        "          break\n",
        "        \n",
        "\n",
        "    ########################\n",
        "    ## PRINT MEAN AFTER EVERY N TIME STEPS\n",
        "    ###################################\n",
        "    if (k % print_every_n == 0):\n",
        "        mean_reward = float(mean_reward) / print_every_n\n",
        "        print(\"K\", k)\n",
        "        print(k, \"th mean_reward=\", mean_reward)\n",
        "        print(\"Epsilon: \", epsilon)\n",
        "        mean_reward_list.append(mean_reward)\n",
        "        mean_reward = 0.0\n",
        "         \n",
        "\n",
        "    #err_list.append(np.mean(trial_err_list))\n",
        "    time_list.append(t + 1)\n",
        "    reward_list.append(acc_reward)  # Store the result\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "print(\"mean_reward_list\", mean_reward_list)\n",
        "reward_file = 'Mean_rewards/'  + str(n_h_neurons) + '_neurons_' + str(n_action) + '_max_objects'\n",
        "np.save(reward_file, mean_reward_list)\n",
        "\n",
        "matplotlib.get_backend()\n",
        "legend_string = str(n_h_neurons) + \" neurons/hidden layer\"\n",
        "plt.plot(np.arange(0,len(mean_reward_list)), mean_reward_list, label = legend_string)\n",
        "plt.legend()\n",
        "print(\"legend_string= \", legend_string)\n",
        "plt.ylabel('Success rate')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylim(0,1)\n",
        "title_string = 'Success rate for agent when counting binaries between 0 and ' + str(n_action) + ' binaries'\n",
        "plt.title(title_string)\n",
        "plt.savefig('Q_learning_binaries.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K 0\n",
            "0 th mean_reward= 0.019\n",
            "Epsilon:  0.999\n",
            "K 50\n",
            "50 th mean_reward= 1.3020000000000007\n",
            "Epsilon:  0.9502544225688344\n",
            "K 100\n",
            "100 th mean_reward= 1.1990000000000007\n",
            "Epsilon:  0.9038873549665959\n",
            "K 150\n",
            "150 th mean_reward= 1.2260000000000006\n",
            "Epsilon:  0.8597827393003539\n",
            "K 200\n",
            "200 th mean_reward= 1.455\n",
            "Epsilon:  0.8178301806491574\n",
            "K 250\n",
            "250 th mean_reward= 1.4050000000000005\n",
            "Epsilon:  0.7779246707428734\n",
            "K 300\n",
            "300 th mean_reward= 1.334000000000001\n",
            "Epsilon:  0.7399663251239436\n",
            "K 350\n",
            "350 th mean_reward= 1.5630000000000004\n",
            "Epsilon:  0.7038601331341691\n",
            "K 400\n",
            "400 th mean_reward= 1.4620000000000002\n",
            "Epsilon:  0.6695157201007336\n",
            "K 450\n",
            "450 th mean_reward= 1.5230000000000006\n",
            "Epsilon:  0.6368471211262058\n",
            "K 500\n",
            "500 th mean_reward= 1.6050000000000002\n",
            "Epsilon:  0.6057725659163237\n",
            "K 550\n",
            "550 th mean_reward= 1.5500000000000003\n",
            "Epsilon:  0.576214274106964\n",
            "K 600\n",
            "600 th mean_reward= 1.5860000000000003\n",
            "Epsilon:  0.548098260578011\n",
            "K 650\n",
            "650 th mean_reward= 1.4680000000000004\n",
            "Epsilon:  0.5213541502668072\n",
            "K 700\n",
            "700 th mean_reward= 1.571\n",
            "Epsilon:  0.4959150020176678\n",
            "K 750\n",
            "750 th mean_reward= 1.6890000000000003\n",
            "Epsilon:  0.47171714102654755\n",
            "K 800\n",
            "800 th mean_reward= 1.6300000000000003\n",
            "Epsilon:  0.44869999946146477\n",
            "K 850\n",
            "850 th mean_reward= 1.6050000000000006\n",
            "Epsilon:  0.4268059648597502\n",
            "K 900\n",
            "900 th mean_reward= 1.6490000000000002\n",
            "Epsilon:  0.4059802359226587\n",
            "K 950\n",
            "950 th mean_reward= 1.6470000000000005\n",
            "Epsilon:  0.38617068534639154\n",
            "K 1000\n",
            "1000 th mean_reward= 1.7710000000000006\n",
            "Epsilon:  0.36732772934619257\n",
            "K 1050\n",
            "1050 th mean_reward= 1.832\n",
            "Epsilon:  0.3494042035469346\n",
            "K 1100\n",
            "1100 th mean_reward= 1.6110000000000002\n",
            "Epsilon:  0.33235524492954527\n",
            "K 1150\n",
            "1150 th mean_reward= 1.775\n",
            "Epsilon:  0.3161381795377863\n",
            "K 1200\n",
            "1200 th mean_reward= 1.7920000000000003\n",
            "Epsilon:  0.3007124156643058\n",
            "K 1250\n",
            "1250 th mean_reward= 1.735\n",
            "Epsilon:  0.28603934224861294\n",
            "K 1300\n",
            "1300 th mean_reward= 1.7920000000000005\n",
            "Epsilon:  0.2720822322326576\n",
            "K 1350\n",
            "1350 th mean_reward= 1.8359999999999999\n",
            "Epsilon:  0.2588061506321157\n",
            "K 1400\n",
            "1400 th mean_reward= 1.8530000000000004\n",
            "Epsilon:  0.2461778670932771\n",
            "K 1450\n",
            "1450 th mean_reward= 1.7940000000000003\n",
            "Epsilon:  0.2341657727166659\n",
            "K 1500\n",
            "1500 th mean_reward= 1.8970000000000002\n",
            "Epsilon:  0.22273980093919937\n",
            "K 1550\n",
            "1550 th mean_reward= 1.834\n",
            "Epsilon:  0.21187135227685275\n",
            "K 1600\n",
            "1600 th mean_reward= 1.8570000000000002\n",
            "Epsilon:  0.2015332227394583\n",
            "K 1650\n",
            "1650 th mean_reward= 1.834\n",
            "Epsilon:  0.1916995357384587\n",
            "K 1700\n",
            "1700 th mean_reward= 1.88\n",
            "Epsilon:  0.18234567731717977\n",
            "K 1750\n",
            "1750 th mean_reward= 1.899\n",
            "Epsilon:  0.17344823454150116\n",
            "K 1800\n",
            "1800 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.1649849368967147\n",
            "K 1850\n",
            "1850 th mean_reward= 1.8359999999999999\n",
            "Epsilon:  0.15693460054388708\n",
            "K 1900\n",
            "1900 th mean_reward= 1.855\n",
            "Epsilon:  0.14927707529619813\n",
            "K 1950\n",
            "1950 th mean_reward= 1.859\n",
            "Epsilon:  0.1419931941825357\n",
            "K 2000\n",
            "2000 th mean_reward= 1.815\n",
            "Epsilon:  0.13506472547210188\n",
            "K 2050\n",
            "2050 th mean_reward= 1.895\n",
            "Epsilon:  0.12847432703995026\n",
            "K 2100\n",
            "2100 th mean_reward= 1.8760000000000003\n",
            "Epsilon:  0.12220550295922675\n",
            "K 2150\n",
            "2150 th mean_reward= 1.8989999999999998\n",
            "Epsilon:  0.11624256221146546\n",
            "K 2200\n",
            "2200 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.11057057941158951\n",
            "K 2250\n",
            "2250 th mean_reward= 1.8570000000000002\n",
            "Epsilon:  0.10517535744931072\n",
            "K 2300\n",
            "2300 th mean_reward= 1.979\n",
            "Epsilon:  0.10004339195341891\n",
            "K 2350\n",
            "2350 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.09516183749001367\n",
            "K 2400\n",
            "2400 th mean_reward= 1.9809999999999999\n",
            "Epsilon:  0.09051847541007228\n",
            "K 2450\n",
            "2450 th mean_reward= 1.979\n",
            "Epsilon:  0.08610168326587533\n",
            "K 2500\n",
            "2500 th mean_reward= 1.9180000000000001\n",
            "Epsilon:  0.08190040571973876\n",
            "K 2550\n",
            "2550 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.07790412687223583\n",
            "K 2600\n",
            "2600 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.07410284394064628\n",
            "K 2650\n",
            "2650 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.07048704222174904\n",
            "K 2700\n",
            "2700 th mean_reward= 1.9369999999999998\n",
            "Epsilon:  0.06704767127628951\n",
            "K 2750\n",
            "2750 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.06377612227551108\n",
            "K 2800\n",
            "2800 th mean_reward= 1.92\n",
            "Epsilon:  0.060664206453048174\n",
            "K 2850\n",
            "2850 th mean_reward= 2.0\n",
            "Epsilon:  0.0577041346082461\n",
            "K 2900\n",
            "2900 th mean_reward= 1.857\n",
            "Epsilon:  0.05488849760960279\n",
            "K 2950\n",
            "2950 th mean_reward= 1.9409999999999998\n",
            "Epsilon:  0.05221024784953349\n",
            "K 3000\n",
            "3000 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.049662681604038215\n",
            "K 3050\n",
            "3050 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.04723942225311833\n",
            "K 3100\n",
            "3100 th mean_reward= 1.979\n",
            "Epsilon:  0.04493440431994225\n",
            "K 3150\n",
            "3150 th mean_reward= 1.979\n",
            "Epsilon:  0.04274185828881007\n",
            "K 3200\n",
            "3200 th mean_reward= 2.0\n",
            "Epsilon:  0.04065629616391608\n",
            "K 3250\n",
            "3250 th mean_reward= 1.979\n",
            "Epsilon:  0.03867249773276234\n",
            "K 3300\n",
            "3300 th mean_reward= 1.92\n",
            "Epsilon:  0.03678549749984046\n",
            "K 3350\n",
            "3350 th mean_reward= 2.0\n",
            "Epsilon:  0.03499057225787607\n",
            "K 3400\n",
            "3400 th mean_reward= 1.9809999999999999\n",
            "Epsilon:  0.03328322926552661\n",
            "K 3450\n",
            "3450 th mean_reward= 2.0\n",
            "Epsilon:  0.031659195001941066\n",
            "K 3500\n",
            "3500 th mean_reward= 1.96\n",
            "Epsilon:  0.030114404470033673\n",
            "K 3550\n",
            "3550 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.02864499102169786\n",
            "K 3600\n",
            "3600 th mean_reward= 1.96\n",
            "Epsilon:  0.027247276679492435\n",
            "K 3650\n",
            "3650 th mean_reward= 1.9809999999999999\n",
            "Epsilon:  0.02591776293057493\n",
            "K 3700\n",
            "3700 th mean_reward= 1.979\n",
            "Epsilon:  0.024653121969839265\n",
            "K 3750\n",
            "3750 th mean_reward= 1.939\n",
            "Epsilon:  0.023450188370338982\n",
            "K 3800\n",
            "3800 th mean_reward= 1.979\n",
            "Epsilon:  0.022305951160147018\n",
            "K 3850\n",
            "3850 th mean_reward= 1.979\n",
            "Epsilon:  0.021217546285819937\n",
            "K 3900\n",
            "3900 th mean_reward= 2.0\n",
            "Epsilon:  0.02018224944360293\n",
            "K 3950\n",
            "3950 th mean_reward= 2.0\n",
            "Epsilon:  0.01919746926043152\n",
            "K 4000\n",
            "4000 th mean_reward= 1.9809999999999999\n",
            "Epsilon:  0.018260740807661956\n",
            "K 4050\n",
            "4050 th mean_reward= 1.979\n",
            "Epsilon:  0.01736971943129524\n",
            "K 4100\n",
            "4100 th mean_reward= 2.0\n",
            "Epsilon:  0.016522174883251375\n",
            "K 4150\n",
            "4150 th mean_reward= 1.9809999999999999\n",
            "Epsilon:  0.01571598573900434\n",
            "K 4200\n",
            "4200 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.014949134087605212\n",
            "K 4250\n",
            "4250 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.01421970048080217\n",
            "K 4300\n",
            "4300 th mean_reward= 2.0\n",
            "Epsilon:  0.01352585912861506\n",
            "K 4350\n",
            "4350 th mean_reward= 1.979\n",
            "Epsilon:  0.012865873329338837\n",
            "K 4400\n",
            "4400 th mean_reward= 1.96\n",
            "Epsilon:  0.012238091122537187\n",
            "K 4450\n",
            "4450 th mean_reward= 1.9809999999999999\n",
            "Epsilon:  0.011640941154145497\n",
            "K 4500\n",
            "4500 th mean_reward= 2.0\n",
            "Epsilon:  0.011072928743333644\n",
            "K 4550\n",
            "4550 th mean_reward= 1.979\n",
            "Epsilon:  0.01053263214128365\n",
            "K 4600\n",
            "4600 th mean_reward= 2.0\n",
            "Epsilon:  0.010018698972517958\n",
            "K 4650\n",
            "4650 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4700\n",
            "4700 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4750\n",
            "4750 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4800\n",
            "4800 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4850\n",
            "4850 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4900\n",
            "4900 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4950\n",
            "4950 th mean_reward= 1.979\n",
            "Epsilon:  0.009998671593271896\n",
            "K 5000\n",
            "5000 th mean_reward= 1.979\n",
            "Epsilon:  0.009998671593271896\n",
            "K 5050\n",
            "5050 th mean_reward= 1.979\n",
            "Epsilon:  "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-ff6ced6f029a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"th mean_reward=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epsilon: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mmean_reward_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mmean_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0;31m# only touch the buffer in the IO thread to avoid races\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;31m# newlines imply flush in subprocesses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0mevent_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    389\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[1;32m    390\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "G40o5CrdOngY",
        "colab_type": "code",
        "outputId": "fdbf7318-aea5-438f-b2eb-73a731a28785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "a=5\n",
        "b=a+2\n",
        "print(\"b\",b)\n",
        "a=7\n",
        "print(\"b\",b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('b', 7)\n",
            "('b', 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SsG2J09Z9-Ne",
        "colab_type": "code",
        "outputId": "2c16bf5a-dcf7-4c8a-fa02-3fbbab9d44ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  observation = []\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "exp_buffer.observation.append(5)\n",
        "exp_buffer.observation.append(7)\n",
        "\n",
        "print(exp_buffer.observation)\n",
        "print(\"Length\", len(exp_buffer.observation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 7]\n",
            "('Length', 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pzIFA6iBiAb7",
        "colab_type": "code",
        "outputId": "8dc9eff1-d0be-405e-e69a-1643cbf245fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  observation = np.empty(0)\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,5)\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,7)\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,9)\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,1)\n",
        "\n",
        "print(exp_buffer.observation)\n",
        "print(\"Length\", exp_buffer.observation.size)\n",
        "\n",
        "\n",
        "randy_arr = np.random.choice(exp_buffer.observation,2)\n",
        "print(\"randy_arr= \", randy_arr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5. 7. 9. 1.]\n",
            "('Length', 4)\n",
            "('randy_arr= ', array([1., 7.]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IHIo8MDYorVp",
        "colab_type": "code",
        "outputId": "3fff3946-6e04-4004-87fe-ba4cf03ef784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  observation = np.array([]).reshape(0,3)\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [1,2,3]])\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [9,9,9]])\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [4,4,4]])\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [1,1,1]])\n",
        "buffer_size = exp_buffer.observation[:,0].size\n",
        "\n",
        "print(exp_buffer.observation)\n",
        "print(\"Length\", buffer_size)\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "#randy_arr = np.random.choice(exp_buffer.observation,2)\n",
        "#print(\"randy_arr= \", randy_arr)\n",
        "\n",
        "#indy = [0,2]\n",
        "indy = random.sample(range(buffer_size), batch_size)\n",
        "print(\"selected vectors: \", exp_buffer.observation[indy])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 3.]\n",
            " [9. 9. 9.]\n",
            " [4. 4. 4.]\n",
            " [1. 1. 1.]]\n",
            "('Length', 4)\n",
            "('selected vectors: ', array([[4., 4., 4.],\n",
            "       [1., 2., 3.]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rzQvHWgqEX00",
        "colab_type": "code",
        "outputId": "1ffb1693-495e-4533-8f2b-23040c1bfffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a = np.empty(0)\n",
        "a = np.append(a, 1)\n",
        "b=a.reshape(batch_size, 1)\n",
        "print(\"b: \", b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b:  [[1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YJxzKWWIX_iV",
        "colab_type": "code",
        "outputId": "16e9f61d-ffdf-4a16-fc85-d8bbe0fa8048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#a = random.sample(range(1), 1)\n",
        "a= range(1)\n",
        "b= random.sample(a, 1)\n",
        "print(\"a= \", a )\n",
        "print(\"b= \", b )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a=  range(0, 1)\n",
            "b=  [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIeM__jYYlcX",
        "colab_type": "code",
        "outputId": "d3cb1ec3-b64a-4f70-e5c0-16e46e662f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "class Restaurant(object):\n",
        "    bankrupt = False\n",
        "    def open_branch(self):\n",
        "        if not self.bankrupt:   #self.bankrupt\n",
        "            print(\"branch opened\")\n",
        "            \n",
        "x = Restaurant()\n",
        "y = Restaurant()\n",
        "y.bankrupt = True\n",
        "\n",
        "print(\"x.bankrupt\", x.bankrupt)\n",
        "x.open_branch()\n",
        "\n",
        "x.a = 5\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x.bankrupt False\n",
            "branch opened\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_QYJjtF8KtUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}