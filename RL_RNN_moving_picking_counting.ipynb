{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy4 of RL_from_NN_to_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "O41vLsJc6zqs",
        "colab_type": "code",
        "outputId": "3f30d5b9-5083-464c-cef1-f6243231def9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## CREATE COUNTABLE BINARY ARRAY with field view !!!!!!\n",
        "#############################\n",
        "\n",
        "import numpy as np\n",
        "import scipy.misc as smp\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "arr_size_half = 20\n",
        "max_N = 1\n",
        "view_size_half = 3\n",
        "object_value = 100.\n",
        "\n",
        "def create_inp_outp_array():\n",
        "    max_objects = 4\n",
        "    n_ones = random.randint(1, max_objects)\n",
        "    one_positions = random.sample(range(view_size_half, view_size_half + max_objects), n_ones)\n",
        "    binary_array = np.zeros(max_objects+2*view_size_half)\n",
        "    binary_array[one_positions] = object_value\n",
        "    #binary_array[2] = object_value\n",
        "    binary_array[0] = 9\n",
        "    binary_array[-1] = 9\n",
        "\n",
        "    return binary_array, n_ones\n",
        "    \n",
        "\n",
        " \n",
        "\n",
        "def make_data_set(n_samples):\n",
        "  mult_inp, mult_out = create_inp_outp_array()\n",
        "  for i in range(n_samples - 1):\n",
        "      single_inp, single_out = create_inp_outp_array()\n",
        "      mult_inp = np.vstack([mult_inp, single_inp])\n",
        "      mult_out = np.vstack((mult_out, single_out))\n",
        "  return mult_inp, mult_out\n",
        "\n",
        "\n",
        "\n",
        "train_input, train_output = make_data_set(10000)\n",
        "max_N=20\n",
        "test_input, test_output = make_data_set(10000)\n",
        "\n",
        "\n",
        "#Print single one\n",
        "inp_ex, out_ex = make_data_set(10)\n",
        "print(\"inp_ex\", inp_ex)\n",
        "print(\"out_ex\", out_ex)\n",
        "\n",
        "\n",
        "#Print set data\n",
        "print(\"===========================\")\n",
        "print(\"train_input: number of examples: \", train_input.shape)\n",
        "print(\"train_input: number of examples: \", train_input[:,0].size)\n",
        "print(\"train_input: single example size: \", train_input[0].size)\n",
        "print(\"train_input: dim of elements in example: \", train_input[0,0].size)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inp_ex [[  9.   0.   0.   0. 100. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0.   0.   0. 100.   0.   0.   0.   9.]\n",
            " [  9.   0.   0.   0.   0. 100.   0.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100.   0. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100.   0. 100.   0.   0.   9.]\n",
            " [  9.   0.   0.   0. 100. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100.   0. 100.   0.   0.   0.   9.]\n",
            " [  9.   0.   0.   0.   0.   0. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100. 100. 100.   0.   0.   9.]]\n",
            "out_ex [[3]\n",
            " [1]\n",
            " [1]\n",
            " [3]\n",
            " [4]\n",
            " [3]\n",
            " [3]\n",
            " [2]\n",
            " [1]\n",
            " [4]]\n",
            "===========================\n",
            "train_input: number of examples:  (10000, 10)\n",
            "train_input: number of examples:  10000\n",
            "train_input: single example size:  10\n",
            "train_input: dim of elements in example:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bp51BbMs8GYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "## UPDATE ENVIRONMENT WHEN MOVING, PICKING WITH LIMITED FIELD VIEW\n",
        "##################################################################\n",
        "\n",
        "\n",
        "def update_env(state, pos, action, reward,t):\n",
        "  \n",
        "  push_finish_button = False\n",
        "  reward=0.0\n",
        "  if(action==0):                      #[0] ----> pick the one at the current position\n",
        "    if(state[pos]==object_value):\n",
        "      state[pos]= 0\n",
        "      reward += point_reward\n",
        "    if(state[pos]==0):\n",
        "      reward += wrong_point_reward     \n",
        "  elif(action==1):                    #[1] ----> stop the current counting process\n",
        "    push_finish_button = True\n",
        "  elif(action==2):                    #[2] ----> go to the left\n",
        "    if(pos>view_size_half):\n",
        "      pos-=1\n",
        "  elif(action==3):                    #[3] ----> go to the right\n",
        "     if(pos<state.size - view_size_half-1):\n",
        "      pos+=1\n",
        "  else:\n",
        "    (\"How could your action go beyond N+3?\")\n",
        "    \n",
        "  if(push_finish_button==False):\n",
        "    reward += time_penalty_reward\n",
        "  if (push_finish_button==True or t==trial_duration-1):\n",
        "    if(state[1]==0 and state[2]==0):\n",
        "      reward += final_reward\n",
        "  if(state[1]==0 and state[2]==0):\n",
        "      reward += final_reward\n",
        "      push_finish_button = True\n",
        "    \n",
        "  return state, pos, push_finish_button, reward \n",
        "    \n",
        "    \n",
        "def get_field_view(state, pos, view_size_half):\n",
        "  fiew_field =  copy.copy(state[pos-view_size_half:pos+view_size_half+1])\n",
        "  return fiew_field\n",
        "\n",
        "def action_int_to_letter(action):\n",
        "  \n",
        "  if(action==0):          #[0] ----> pick the one at the current position\n",
        "     act_str = \"P\"\n",
        "  elif(action==1):        #[1] ----> stop the current counting proces\n",
        "     act_str = \"STOP\"\n",
        "  elif(action==2):        #[2] ----> go to the left\n",
        "    act_str = \"L\"\n",
        "  elif(action==3):        #[3] ----> go to the right\n",
        "    act_str = \"R\"\n",
        "  else:\n",
        "    (\"How could your action go beyond N+3?\")\n",
        "    \n",
        "  return act_str\n",
        "    \n",
        "\n",
        "def get_animation_pic(state,pos,view_size_half,action):\n",
        "  s = \" \"\n",
        "  for i in range(pos-view_size_half):\n",
        "    s+=\"   \"\n",
        "  s += \"|\"\n",
        "  for i in range(view_size_half):\n",
        "    s+=\"__\"\n",
        "  s += \"I\"\n",
        "  for i in range(view_size_half):\n",
        "    s+=\"__\"\n",
        "  s += \"|\"\n",
        "  for i in range(state.size - (pos-view_size_half+3) ):\n",
        "    s+=\"   \"\n",
        "  s+= \"  \"\n",
        "  act_str = action_int_to_letter(action)\n",
        "  s+= act_str\n",
        "  print(state)\n",
        "  print(s)\n",
        "  \n",
        "  \n",
        "\n",
        "  \n",
        "class ExperienceBuffer:\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.observation = np.array([]).reshape(0,dim_obs)\n",
        "    self.new_observation = np.array([]).reshape(0,dim_obs)\n",
        "    self.action = np.empty(0)\n",
        "    self.push_finish_button = np.empty(0)\n",
        "    self.reward = np.empty(0)\n",
        "  \n",
        "  \n",
        "  def add_exp(self, observation, new_observation, action, reward, push_finish_button):\n",
        "  \n",
        "    if(len(exp_buffer.observation) < batch_size):  \n",
        "\n",
        "        self.observation = np.vstack([exp_buffer.observation, observation])\n",
        "        self.new_observation = np.vstack([exp_buffer.new_observation, new_observation])\n",
        "        self.action = np.append(exp_buffer.action, action)\n",
        "        self.reward = np.append(exp_buffer.reward, reward)\n",
        "        self.push_finish_button = np.append(exp_buffer.push_finish_button, np.float32(push_finish_button))\n",
        "\n",
        "    else:\n",
        "        rand_i = random.randint(0,batch_size-1)\n",
        "        self.observation[rand_i] = observation\n",
        "        self.new_observation[rand_i] = new_observation\n",
        "        self.action[rand_i] = action\n",
        "        self.reward[rand_i] = reward\n",
        "        self.push_finish_button[rand_i] = np.float32(push_finish_button)\n",
        "        \n",
        "    \n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tO5pXpE5frNJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################\n",
        "## SET PARAMETERS\n",
        "########################\n",
        "# RL Parameters\n",
        "\n",
        "point_reward = +0.1\n",
        "wrong_point_reward = -0.0\n",
        "time_penalty_reward = -0.05\n",
        "final_reward = +1.0\n",
        "stop_reward = +0.0\n",
        "trial_duration = 10\n",
        "\n",
        "\n",
        "# NETWORK LEARNING PARAMETERS\n",
        "batch_size = 1\n",
        "buffer_size = 1\n",
        "gamma = 1.0\n",
        "epsilon_init = 1.\n",
        "learning_rate_init = 3e-4\n",
        "epsi_decay = .999\n",
        "lr_decay = .999\n",
        "epsilon_min = 0.01\n",
        "learning_min = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NfZJIp7sEtOy",
        "colab_type": "code",
        "outputId": "bd232043-cfd6-429d-c432-a8136bcce426",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2628
        }
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "#import gym\n",
        "import numpy as np\n",
        "import numpy.random as rd\n",
        "import matplotlib.pyplot as plt\n",
        "#from gym.envs.classic_control.cartpole import CartPoleEnv\n",
        "#from cartpole_utils import plot_results,print_results\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def init_weights(shape, name):\n",
        "    return tf.Variable(tf.random_normal(shape, stddev=0.01)/ np.sqrt(dim_obs), name=name)\n",
        "\n",
        "\n",
        "def Q_network(state_holder, w_h, w_h2, w_o, b, b_h):\n",
        "    h = tf.nn.relu(tf.matmul(state_holder, w_h) + b_h )\n",
        "    #h2 = tf.nn.relu(tf.matmul(h, w_h2))\n",
        "    Q =  tf.matmul(h, w_o, name='output_activation') + b \n",
        "    #Q = tf.nn.sigmoid(a_z, name='Q_model')\n",
        "    return Q\n",
        "  \n",
        "def Q_RNN_Network(cell, obs_holder, net_state_sess, weight, bias):\n",
        "  val, next_net_state = tf.nn.dynamic_rnn(cell, obs_holder, initial_state=net_state_sess, dtype=tf.float32)\n",
        "  val = tf.transpose(val, [1, 0, 2])\n",
        "  last = tf.gather(val, int(val.get_shape()[0]) - 1)\n",
        "  Q_i = tf.matmul(last, weight) + bias\n",
        "  return Q_i, next_net_state\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load Input=Start-of-a-game &&  Classes=Check-for-reward\n",
        "trX = train_input\n",
        "trY = train_output\n",
        "teX = test_input\n",
        "teY = test_output\n",
        "\n",
        "\n",
        "\n",
        "## SET ENVIRONMENT + #ofActions #ofStates\n",
        "n_action = 4     #[Pick, Stop, Left, Right]\n",
        "dim_obs = 1+2*view_size_half\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## BUILD TENSORFLOW\n",
        "#############################\n",
        "\n",
        "\n",
        "# PLACEHOLDER\n",
        "action_holder = tf.placeholder(dtype=tf.int32, shape=(None, 1), name=\"action_holder\")  # +1 because that is the node responsible to say if it is done ore not\n",
        "state_holder = tf.placeholder(dtype=tf.float32, shape=(None, dim_obs), name='state_holder')\n",
        "next_state_holder = tf.placeholder(dtype=tf.float32, shape=(None, dim_obs), name='next_state_holder')\n",
        "\n",
        "\n",
        "#PLACEHOLDER\n",
        "r_holder = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='r_holder')\n",
        "is_done_holder = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='is_done_holder')\n",
        "learning_rate_holder = tf.placeholder(dtype=tf.float32, name='symbolic_state')\n",
        "\n",
        "\n",
        "# Q - NETWORK\n",
        "n_h_neurons = 20\n",
        "w_h = init_weights([dim_obs, n_h_neurons], \"w_h\")\n",
        "w_h2 = init_weights([n_h_neurons, n_h_neurons], \"w_h2\")\n",
        "w_o = init_weights([n_h_neurons, n_action], \"w_o\")\n",
        "b0 = np.zeros(n_action, dtype=np.float32)\n",
        "b_h0 = np.zeros(n_h_neurons, dtype=np.float32)\n",
        "b = tf.Variable(initial_value=b0, trainable=True, name='bias')\n",
        "b_h = tf.Variable(initial_value=b_h0, trainable=True, name='bias')\n",
        "\n",
        "\n",
        "# Q - RNN NETWORK\n",
        "# Initialize the parameters of the Q-RNN model\n",
        "num_hidden = 20\n",
        "weight = tf.Variable(tf.truncated_normal([num_hidden, n_action]))\n",
        "bias = tf.Variable(tf.constant(0.1, shape=[n_action]))\n",
        "\n",
        "cell = tf.contrib.rnn.LSTMCell(num_hidden,state_is_tuple=True,reuse=tf.AUTO_REUSE)\n",
        "\n",
        "net_state_sess = cell.zero_state(batch_size, tf.float32)\n",
        "next_net_state = cell.zero_state(batch_size, tf.float32)\n",
        "\n",
        "\n",
        "#############################\n",
        "\n",
        "\n",
        "###################\n",
        "### CALCULATE Q\n",
        "################\n",
        "Q = Q_network(state_holder, w_h, w_h2, w_o,b, b_h)\n",
        "next_Q = Q_network(next_state_holder, w_h, w_h2, w_o,b, b_h)\n",
        "#Q, next_net_state = Q_RNN_Network(cell, obs_holder, net_state_sess, weight, bias)\n",
        "#next_Q = Q_RNN_Network(cell, next_obs_holder, net_state_sess, weight, bias)\n",
        "\n",
        "\n",
        "\n",
        "### COMPARE REWARD<->PREDICTED REWARD ===> LOSS ####\n",
        "\n",
        "#### From https://lilianweng.github.io/lil-log/2018/05/05/implementing-deep-reinforcement-learning-models.html\n",
        "# The prediction by the primary Q network for the actual actions.\n",
        "action_one_hot = tf.one_hot(action_holder, n_action, 1.0, 0.0, name='action_one_hot')\n",
        "R = tf.reduce_sum(Q * action_one_hot, reduction_indices=-1, name='q_acted')\n",
        "\n",
        "# The optimization target defined by the Bellman equation and the target network.\n",
        "max_q_next_by_target = tf.reduce_max(next_Q, axis=-1)\n",
        "R_next = r_holder + (1. - is_done_holder) * gamma * max_q_next_by_target\n",
        "\n",
        "# The loss measures the mean squared error between prediction and target.\n",
        "err = tf.reduce_mean(tf.square(R - tf.stop_gradient(R_next)), name=\"loss_mse_train\")\n",
        "training_step = tf.train.AdamOptimizer(learning_rate=learning_rate_holder).minimize(err, name=\"adam_optim\")\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sess = tf.Session()  # FOR NOW everything is symbolic, this object has to be called to compute each value of Q\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "################################\n",
        "################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################\n",
        "## PI(S)\n",
        "## First check if its random time THEN\n",
        "        # - calculate State--Network--> Q\n",
        "        # - take maximum of Q\n",
        "        # - just remember and return index of maximum    ---> returns ACTION that leads to max_Q-value (not value itself)\n",
        "################################\n",
        "\n",
        "def policy(observation):\n",
        "    Q_values = sess.run(Q, feed_dict={state_holder: observation.reshape(1, dim_obs)} )\n",
        "    #Q_values, net_state = sess.run((Q, next_net_state), feed_dict={obs_holder: obs.reshape(1, dim_obs,1), net_state_sess: net_state})\n",
        "\n",
        "    Q_values_reduced = Q_values[0, :]\n",
        "    val = np.max(Q_values_reduced)  # Change in dimension\n",
        "    max_indices = np.where(Q_values_reduced == val)[0]\n",
        "    \n",
        "    action = 0\n",
        "    if rd.rand() < epsilon:\n",
        "      action = rd.randint(0, n_action)\n",
        "    else:\n",
        "      action = rd.choice(max_indices)\n",
        "\n",
        "    return action \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## ACTUAL RUN\n",
        "###########################\n",
        "\n",
        "time_list = []\n",
        "reward_list = []\n",
        "err_list = []\n",
        "val_list = []\n",
        "trial_err_list = []\n",
        "mean_reward_list = []\n",
        "\n",
        "mean_reward = 0\n",
        "print_every_n = 50\n",
        "epsilon = epsilon_init\n",
        "learning_rate = learning_rate_init\n",
        "n_episodes = trX[:, 0].size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "\n",
        "for k in range(n_episodes):\n",
        "\n",
        "    acc_reward = 0                  # Init the accumulated reward\n",
        "    push_finish_button = False\n",
        "    internal_counter_memory = 0\n",
        "    \n",
        "    # Initialize external and internal state of agent\n",
        "    k_n = rd.randint(0, trX[:,0].size)                                            ######   !!!!!!!!!!!!!!!!!\n",
        "    state = copy.copy(trX[k_n, :])\n",
        "    pos = view_size_half\n",
        "    observation = get_field_view(state, pos, view_size_half)\n",
        "    net_state = sess.run(net_state_sess)\n",
        "        \n",
        "    \n",
        "    for t in range(trial_duration):  \n",
        "        \n",
        "\n",
        "        old_observation = copy.copy(observation)\n",
        "        old_internal_counter_memory = copy.copy(internal_counter_memory)\n",
        "        state_copy = copy.copy(state)\n",
        "\n",
        "        #####################\n",
        "        ## Chose action by Random or Maximum Q-value\n",
        "        ################################\n",
        "          \n",
        "        action = policy(observation)\n",
        "          \n",
        "        ######################\n",
        "        ## Update Environment, get reward according to chosen action\n",
        "        #################################################    \n",
        "        new_state, pos, push_finish_button, reward = update_env(state_copy, pos, action, reward, t)\n",
        "        new_observation = get_field_view(new_state, pos, view_size_half)\n",
        "        exp_buffer.add_exp(observation, new_observation, action, reward, push_finish_button)\n",
        "\n",
        "\n",
        "        \n",
        "        ############################\n",
        "        ## TRAINING STEP, ERROR CALCULATION (RUN SESSION)\n",
        "        ############################\n",
        "                \n",
        "        \n",
        "        buffer_size = exp_buffer.observation[:,0].size\n",
        "        \n",
        "        if(buffer_size >= batch_size):\n",
        "     \n",
        "          error, R_out, R_next_out = sess.run([err, R, R_next], feed_dict={\n",
        "              state_holder: exp_buffer.observation.reshape(batch_size, dim_obs),\n",
        "              next_state_holder: exp_buffer.new_observation.reshape(batch_size, dim_obs) ,\n",
        "              action_holder: exp_buffer.action.reshape(batch_size, 1),\n",
        "              is_done_holder: exp_buffer.push_finish_button.reshape(batch_size, 1),\n",
        "              r_holder: exp_buffer.reward.reshape(batch_size, 1)})   \n",
        "\n",
        "          sess.run(training_step, feed_dict={\n",
        "              state_holder: exp_buffer.observation.reshape(batch_size, dim_obs),\n",
        "              next_state_holder: exp_buffer.new_observation.reshape(batch_size, dim_obs),\n",
        "              action_holder: exp_buffer.action.reshape(batch_size, 1),\n",
        "              is_done_holder: exp_buffer.push_finish_button.reshape(batch_size, 1),\n",
        "              r_holder: exp_buffer.reward.reshape(batch_size, 1),\n",
        "              learning_rate_holder: learning_rate})\n",
        "        \n",
        "\n",
        "          if(epsilon>epsilon_min):\n",
        "             epsilon *= epsi_decay\n",
        "          if (learning_rate > learning_min):\n",
        "             learning_rate *= lr_decay\n",
        "        \n",
        "\n",
        "        observation = new_observation  # Pass the new state to the next step\n",
        "        old_state = copy.copy(state)\n",
        "        state = new_state\n",
        "        acc_reward += reward  # Accumulate the reward\n",
        "        #trial_err_list.append(error)\n",
        "\n",
        "        \n",
        "        ########################\n",
        "        ## PRINT IN SINGLE TIME STEPS\n",
        "        #########################\n",
        "                    \n",
        "        if( (k>=0 and k<9) and (k>=700 and k<709) ):\n",
        "          if(t==0):\n",
        "            print(\"\\n ================\")\n",
        "            print(\"K= \", k)\n",
        "          print(\"t: \", t)\n",
        "          print(\"Action\", action)\n",
        "          get_animation_pic(state,pos,view_size_half,action)\n",
        "          print(\"observation 2: \", observation)      \n",
        "          print(\"Reward: \", reward)\n",
        "          print(\"--------------\")\n",
        "         \n",
        "        if (push_finish_button==True or t==trial_duration-1):\n",
        "          mean_reward += acc_reward\n",
        "          break\n",
        "        \n",
        "\n",
        "    ########################\n",
        "    ## PRINT MEAN AFTER EVERY N TIME STEPS\n",
        "    ###################################\n",
        "    if (k % print_every_n == 0):\n",
        "        mean_reward = float(mean_reward) / print_every_n\n",
        "        print(\"K\", k)\n",
        "        print(k, \"th mean_reward=\", mean_reward)\n",
        "        print(\"Epsilon: \", epsilon)\n",
        "        mean_reward_list.append(mean_reward)\n",
        "        mean_reward = 0.0\n",
        "         \n",
        "\n",
        "    #err_list.append(np.mean(trial_err_list))\n",
        "    time_list.append(t + 1)\n",
        "    reward_list.append(acc_reward)  # Store the result\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "print(\"mean_reward_list\", mean_reward_list)\n",
        "reward_file = 'Mean_rewards/'  + str(n_h_neurons) + '_neurons_' + str(n_action) + '_max_objects'\n",
        "np.save(reward_file, mean_reward_list)\n",
        "\n",
        "matplotlib.get_backend()\n",
        "legend_string = str(n_h_neurons) + \" neurons/hidden layer\"\n",
        "plt.plot(np.arange(0,len(mean_reward_list)), mean_reward_list, label = legend_string)\n",
        "plt.legend()\n",
        "print(\"legend_string= \", legend_string)\n",
        "plt.ylabel('Success rate')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylim(0,1)\n",
        "title_string = 'Success rate for agent when counting binaries between 0 and ' + str(n_action) + ' binaries'\n",
        "plt.title(title_string)\n",
        "plt.savefig('Q_learning_binaries.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "K 0\n",
            "0 th mean_reward= 0.04\n",
            "Epsilon:  0.999\n",
            "K 50\n",
            "50 th mean_reward= 1.1860000000000004\n",
            "Epsilon:  0.9502544225688344\n",
            "K 100\n",
            "100 th mean_reward= 1.1190000000000007\n",
            "Epsilon:  0.9038873549665959\n",
            "K 150\n",
            "150 th mean_reward= 1.2790000000000004\n",
            "Epsilon:  0.8597827393003539\n",
            "K 200\n",
            "200 th mean_reward= 1.4010000000000007\n",
            "Epsilon:  0.8178301806491574\n",
            "K 250\n",
            "250 th mean_reward= 1.3820000000000008\n",
            "Epsilon:  0.7779246707428734\n",
            "K 300\n",
            "300 th mean_reward= 1.4410000000000007\n",
            "Epsilon:  0.7399663251239436\n",
            "K 350\n",
            "350 th mean_reward= 1.4720000000000004\n",
            "Epsilon:  0.7038601331341691\n",
            "K 400\n",
            "400 th mean_reward= 1.4660000000000004\n",
            "Epsilon:  0.6695157201007336\n",
            "K 450\n",
            "450 th mean_reward= 1.4410000000000005\n",
            "Epsilon:  0.6368471211262058\n",
            "K 500\n",
            "500 th mean_reward= 1.5230000000000006\n",
            "Epsilon:  0.6057725659163237\n",
            "K 550\n",
            "550 th mean_reward= 1.5480000000000007\n",
            "Epsilon:  0.576214274106964\n",
            "K 600\n",
            "600 th mean_reward= 1.659\n",
            "Epsilon:  0.548098260578011\n",
            "K 650\n",
            "650 th mean_reward= 1.5270000000000001\n",
            "Epsilon:  0.5213541502668072\n",
            "K 700\n",
            "700 th mean_reward= 1.5880000000000007\n",
            "Epsilon:  0.4959150020176678\n",
            "K 750\n",
            "750 th mean_reward= 1.6110000000000004\n",
            "Epsilon:  0.47171714102654755\n",
            "K 800\n",
            "800 th mean_reward= 1.7940000000000003\n",
            "Epsilon:  0.44869999946146477\n",
            "K 850\n",
            "850 th mean_reward= 1.7310000000000005\n",
            "Epsilon:  0.4268059648597502\n",
            "K 900\n",
            "900 th mean_reward= 1.6760000000000002\n",
            "Epsilon:  0.4059802359226587\n",
            "K 950\n",
            "950 th mean_reward= 1.6740000000000004\n",
            "Epsilon:  0.38617068534639154\n",
            "K 1000\n",
            "1000 th mean_reward= 1.6490000000000002\n",
            "Epsilon:  0.36732772934619257\n",
            "K 1050\n",
            "1050 th mean_reward= 1.6930000000000005\n",
            "Epsilon:  0.3494042035469346\n",
            "K 1100\n",
            "1100 th mean_reward= 1.7730000000000004\n",
            "Epsilon:  0.33235524492954527\n",
            "K 1150\n",
            "1150 th mean_reward= 1.8530000000000002\n",
            "Epsilon:  0.3161381795377863\n",
            "K 1200\n",
            "1200 th mean_reward= 1.6720000000000002\n",
            "Epsilon:  0.3007124156643058\n",
            "K 1250\n",
            "1250 th mean_reward= 1.775\n",
            "Epsilon:  0.28603934224861294\n",
            "K 1300\n",
            "1300 th mean_reward= 1.7330000000000003\n",
            "Epsilon:  0.2720822322326576\n",
            "K 1350\n",
            "1350 th mean_reward= 1.7520000000000004\n",
            "Epsilon:  0.2588061506321157\n",
            "K 1400\n",
            "1400 th mean_reward= 1.9409999999999998\n",
            "Epsilon:  0.2461778670932771\n",
            "K 1450\n",
            "1450 th mean_reward= 1.8970000000000002\n",
            "Epsilon:  0.2341657727166659\n",
            "K 1500\n",
            "1500 th mean_reward= 1.838\n",
            "Epsilon:  0.22273980093919937\n",
            "K 1550\n",
            "1550 th mean_reward= 1.697\n",
            "Epsilon:  0.21187135227685275\n",
            "K 1600\n",
            "1600 th mean_reward= 1.8360000000000003\n",
            "Epsilon:  0.2015332227394583\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-cc34c0bfedbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m               \u001b[0mis_done_holder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexp_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_finish_button\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m               \u001b[0mr_holder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexp_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m               learning_rate_holder: learning_rate})\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "G40o5CrdOngY",
        "colab_type": "code",
        "outputId": "fdbf7318-aea5-438f-b2eb-73a731a28785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "a=5\n",
        "b=a+2\n",
        "print(\"b\",b)\n",
        "a=7\n",
        "print(\"b\",b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('b', 7)\n",
            "('b', 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SsG2J09Z9-Ne",
        "colab_type": "code",
        "outputId": "2c16bf5a-dcf7-4c8a-fa02-3fbbab9d44ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  observation = []\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "exp_buffer.observation.append(5)\n",
        "exp_buffer.observation.append(7)\n",
        "\n",
        "print(exp_buffer.observation)\n",
        "print(\"Length\", len(exp_buffer.observation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 7]\n",
            "('Length', 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pzIFA6iBiAb7",
        "colab_type": "code",
        "outputId": "8dc9eff1-d0be-405e-e69a-1643cbf245fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  observation = np.empty(0)\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,5)\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,7)\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,9)\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,1)\n",
        "\n",
        "print(exp_buffer.observation)\n",
        "print(\"Length\", exp_buffer.observation.size)\n",
        "\n",
        "\n",
        "randy_arr = np.random.choice(exp_buffer.observation,2)\n",
        "print(\"randy_arr= \", randy_arr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5. 7. 9. 1.]\n",
            "('Length', 4)\n",
            "('randy_arr= ', array([1., 7.]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IHIo8MDYorVp",
        "colab_type": "code",
        "outputId": "3fff3946-6e04-4004-87fe-ba4cf03ef784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  observation = np.array([]).reshape(0,3)\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [1,2,3]])\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [9,9,9]])\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [4,4,4]])\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [1,1,1]])\n",
        "buffer_size = exp_buffer.observation[:,0].size\n",
        "\n",
        "print(exp_buffer.observation)\n",
        "print(\"Length\", buffer_size)\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "#randy_arr = np.random.choice(exp_buffer.observation,2)\n",
        "#print(\"randy_arr= \", randy_arr)\n",
        "\n",
        "#indy = [0,2]\n",
        "indy = random.sample(range(buffer_size), batch_size)\n",
        "print(\"selected vectors: \", exp_buffer.observation[indy])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 3.]\n",
            " [9. 9. 9.]\n",
            " [4. 4. 4.]\n",
            " [1. 1. 1.]]\n",
            "('Length', 4)\n",
            "('selected vectors: ', array([[4., 4., 4.],\n",
            "       [1., 2., 3.]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rzQvHWgqEX00",
        "colab_type": "code",
        "outputId": "1ffb1693-495e-4533-8f2b-23040c1bfffb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "a = np.empty(0)\n",
        "a = np.append(a, 1)\n",
        "b=a.reshape(batch_size, 1)\n",
        "print(\"b: \", b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b:  [[1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YJxzKWWIX_iV",
        "colab_type": "code",
        "outputId": "16e9f61d-ffdf-4a16-fc85-d8bbe0fa8048",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "#a = random.sample(range(1), 1)\n",
        "a= range(1)\n",
        "b= random.sample(a, 1)\n",
        "print(\"a= \", a )\n",
        "print(\"b= \", b )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a=  range(0, 1)\n",
            "b=  [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIeM__jYYlcX",
        "colab_type": "code",
        "outputId": "d3cb1ec3-b64a-4f70-e5c0-16e46e662f6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "class Restaurant(object):\n",
        "    bankrupt = False\n",
        "    def open_branch(self):\n",
        "        if not self.bankrupt:   #self.bankrupt\n",
        "            print(\"branch opened\")\n",
        "            \n",
        "x = Restaurant()\n",
        "y = Restaurant()\n",
        "y.bankrupt = True\n",
        "\n",
        "print(\"x.bankrupt\", x.bankrupt)\n",
        "x.open_branch()\n",
        "\n",
        "x.a = 5\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x.bankrupt False\n",
            "branch opened\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_QYJjtF8KtUE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}