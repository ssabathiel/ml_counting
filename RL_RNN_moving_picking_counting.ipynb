{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of RL_from_NN_to_RNN.ipynb","version":"0.3.2","provenance":[{"file_id":"1TpsoSK8JiPBtTxKDCpywBZbQQh-oOEsM","timestamp":1547042438218}],"collapsed_sections":[]},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"metadata":{"id":"O41vLsJc6zqs","colab_type":"code","outputId":"e13e450d-8f28-45ec-8200-f278ac4fa6e0","executionInfo":{"status":"ok","timestamp":1547129684594,"user_tz":-60,"elapsed":579,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}},"colab":{"base_uri":"https://localhost:8080/","height":442}},"cell_type":"code","source":["##############################\n","## CREATE COUNTABLE BINARY ARRAY with field view !!!!!!\n","#############################\n","\n","import numpy as np\n","import scipy.misc as smp\n","import random\n","\n","\n","\n","arr_size_half = 20\n","max_N = 1\n","view_size_half = 1\n","\n","def create_inp_outp_array():\n","    max_objects = 3\n","    n_ones = random.randint(1, max_objects)\n","    one_positions = random.sample(range(view_size_half, view_size_half + max_objects), n_ones)\n","    binary_array = np.zeros(max_objects+2*view_size_half)\n","    binary_array[one_positions] = 1\n","    binary_array[0] = 5\n","    binary_array[-1] = 5\n","\n","    return binary_array, n_ones\n","    \n","\n"," \n","\n","def make_data_set(n_samples):\n","  mult_inp, mult_out = create_inp_outp_array()\n","  for i in range(n_samples - 1):\n","      single_inp, single_out = create_inp_outp_array()\n","      mult_inp = np.vstack([mult_inp, single_inp])\n","      mult_out = np.vstack((mult_out, single_out))\n","  return mult_inp, mult_out\n","\n","\n","\n","train_input, train_output = make_data_set(1000)\n","max_N=20\n","test_input, test_output = make_data_set(1000)\n","\n","\n","#Print single one\n","inp_ex, out_ex = make_data_set(10)\n","print(\"inp_ex\", inp_ex)\n","print(\"out_ex\", out_ex)\n","\n","\n","#Print set data\n","print(\"===========================\")\n","print(\"train_input: number of examples: \", train_input.shape)\n","print(\"train_input: number of examples: \", train_input[:,0].size)\n","print(\"train_input: single example size: \", train_input[0].size)\n","print(\"train_input: dim of elements in example: \", train_input[0,0].size)\n","\n","\n"],"execution_count":81,"outputs":[{"output_type":"stream","text":["('inp_ex', array([[5., 1., 0., 0., 5.],\n","       [5., 1., 1., 1., 5.],\n","       [5., 0., 1., 0., 5.],\n","       [5., 0., 1., 1., 5.],\n","       [5., 1., 0., 1., 5.],\n","       [5., 0., 1., 0., 5.],\n","       [5., 1., 0., 1., 5.],\n","       [5., 1., 1., 1., 5.],\n","       [5., 1., 1., 0., 5.],\n","       [5., 1., 1., 1., 5.]]))\n","('out_ex', array([[1],\n","       [3],\n","       [1],\n","       [2],\n","       [2],\n","       [1],\n","       [2],\n","       [3],\n","       [2],\n","       [3]]))\n","===========================\n","('train_input: number of examples: ', (1000, 5))\n","('train_input: number of examples: ', 1000)\n","('train_input: single example size: ', 5)\n","('train_input: dim of elements in example: ', 1)\n"],"name":"stdout"}]},{"metadata":{"id":"bp51BbMs8GYq","colab_type":"code","colab":{}},"cell_type":"code","source":["########################################\n","## UPDATE ENVIRONMENT WHEN MOVING, PICKING WITH LIMITED FIELD VIEW\n","##################################################################\n","\n","\n","def update_env(state, pos, action, reward):\n","  \n","  push_finish_button = False\n","  #reward=0.0\n","  if(action==0):        #[N+2] ----> pick the one at the current position\n","    if(state[pos]==1):\n","      state[pos]= 0\n","      reward += point_reward\n","  elif(action==1):        #[N+3] ----> stop the current counting process\n","    push_finish_button = True\n","  elif(action==2):          #[N] ----> go to the left\n","    if(pos>view_size_half):\n","      pos-=1\n","  elif(action==3):        #[N+1] ----> go to the right\n","     if(pos<state.size - view_size_half-1):\n","      pos+=1\n","  else:\n","    (\"How could your action go beyond N+3?\")\n","    \n","  return state, pos, push_finish_button, reward \n","    \n","    \n","def get_field_view(state, pos, view_size_half):\n","  fiew_field =  copy.copy(state[pos-view_size_half:pos+view_size_half+1])\n","  return fiew_field\n","\n","def action_int_to_letter(action):\n","  \n","  if(action==0):          #[N] ----> go to the left\n","     act_str = \"P\"\n","  elif(action==1):        #[N+1] ----> go to the Right\n","     act_str = \"STOP\"\n","  elif(action==2):        #[N+2] ----> pick the one at the current position\n","    act_str = \"L\"\n","  elif(action==3):        #[N+3] ----> stop the current counting process\n","    act_str = \"R\"\n","  else:\n","    (\"How could your action go beyond N+3?\")\n","    \n","  return act_str\n","    \n","\n","def get_animation_pic(state,pos,view_size_half,action):\n","  s = \" \"\n","  for i in range(pos-view_size_half):\n","    s+=\"   \"\n","  s += \"|\"\n","  for i in range(view_size_half):\n","    s+=\"__\"\n","  s += \"I\"\n","  for i in range(view_size_half):\n","    s+=\"__\"\n","  s += \"|\"\n","  for i in range(state.size - (pos-view_size_half+3) ):\n","    s+=\"   \"\n","  s+= \"  \"\n","  act_str = action_int_to_letter(action)\n","  s+= act_str\n","  print(state)\n","  print(s)\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"NfZJIp7sEtOy","colab_type":"code","outputId":"a0befb6b-c539-4924-da41-9b9535fb65b1","executionInfo":{"status":"error","timestamp":1547129723944,"user_tz":-60,"elapsed":24143,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}},"colab":{"base_uri":"https://localhost:8080/","height":4586}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","#import gym\n","import numpy as np\n","import numpy.random as rd\n","import matplotlib.pyplot as plt\n","#from gym.envs.classic_control.cartpole import CartPoleEnv\n","#from cartpole_utils import plot_results,print_results\n","import tensorflow as tf\n","import copy\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","\n","def init_weights(shape, name):\n","    return tf.Variable(tf.random_normal(shape, stddev=0.01)/ np.sqrt(dim_state), name=name)\n","\n","\n","def Q_network(state_holder, w_h, w_h2, w_o, b):\n","    h = tf.nn.relu(tf.matmul(state_holder, w_h))\n","    #h2 = tf.nn.relu(tf.matmul(h, w_h2))\n","    Q = tf.matmul(h, w_o, name='output_activation') + b\n","    #Q = tf.nn.sigmoid(a_z, name='Q_model')\n","    return Q\n","\n","\n","########################\n","## SET PARAMETERS\n","########################\n","# Algorithm parameters\n","\n","point_reward = +10.0\n","time_penalty_reward = -0.2\n","final_reward = +0.0\n","stop_reward = +0.0\n","\n","\n","\n","learning_rate = 3e-3\n","gamma = 1.0\n","epsilon = 1.\n","epsi_decay = .995\n","lr_decay = .995\n","epsilon_min = 0.01\n","learning_min = 0.00001\n","\n","\n","# Load Input=Start-of-a-game &&  Classes=Check-for-reward\n","trX = train_input\n","trY = train_output\n","teX = test_input\n","teY = test_output\n","\n","print(trX.shape)\n","print(trY.shape)\n","print(teX.shape)\n","print(teY.shape)\n","\n","\n","## SET ENVIRONMENT + #ofActions #ofStates\n","dim_state = trX[0,:].size\n","n_action = 4     #[Pick, Stop, Left, Right]\n","dim_obs = 1+2*view_size_half\n","N_print_every = 100\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","##############################\n","## BUILD TENSORFLOW\n","#############################\n","\n","\n","# PLACEHOLDER\n","action_holder = tf.placeholder(dtype=tf.int32, name=\"Y\")  # +1 because that is the node responsible to say if it is done ore not\n","state_holder = tf.placeholder(dtype=tf.float32, shape=(1, dim_obs), name='symbolic_state')\n","next_state_holder = tf.placeholder(dtype=tf.float32, shape=(1, dim_obs), name='symbolic_state')\n","\n","# Initialize the parameters of the Q model\n","n_h_neurons = 20\n","w_h = init_weights([dim_obs, n_h_neurons], \"w_h\")\n","w_h2 = init_weights([n_h_neurons, n_h_neurons], \"w_h2\")\n","w_o = init_weights([n_h_neurons, n_action], \"w_o\")\n","b0 = np.zeros(n_action, dtype=np.float32)\n","b = tf.Variable(initial_value=b0, trainable=True, name='bias')\n","\n","Q = Q_network(state_holder, w_h, w_h2, w_o,b)\n","next_Q = Q_network(next_state_holder, w_h, w_h2, w_o,b)\n","\n","\n","### COMPARE REWARDS ####\n","#PLACEHOLDER\n","r_holder = tf.placeholder(dtype=tf.float32, name='symbolic_value_estimation')\n","is_done_holder = tf.placeholder(dtype=tf.float32, name='is_done')\n","\n","next_action_Q_max = tf.to_int32(tf.argmax(next_Q, dimension=1, name='next_step_maxQ_action'))           # [ 0.1   0.4   0.8  0.8] -->  3, 4\n","R = Q[0, action_holder]\n","next_R = r_holder + gamma * next_Q[0, next_action_Q_max[0]] * (1 - is_done_holder)\n","error = (R - next_R)**2\n","\n","# Define the operation that performs the optimization\n","learning_rate_holder = tf.placeholder(dtype=tf.float32, name='symbolic_state')\n","training_step = tf.train.GradientDescentOptimizer(learning_rate_holder).minimize(error)\n","\n","sess = tf.Session()  # FOR NOW everything is symbolic, this object has to be called to compute each value of Q\n","sess.run(tf.initialize_all_variables())\n","\n","################################\n","################################\n","\n","\n","\n","\n","\n","################################\n","## PI(S)\n","## First check if its random time THEN\n","        # - calculate State--Network--> Q\n","        # - take maximum of Q\n","        # - just remember and return index of maximum    ---> returns ACTION that leads to max_Q-value (not value itself)\n","################################\n","\n","def policy(state):\n","    if rd.rand() < epsilon:\n","        return rd.randint(0, n_action)\n","    reshaped_state = state.reshape(1, dim_state)\n","    Q_values = sess.run(Q, feed_dict={state_holder: state.reshape(1, dim_state)})\n","    Q_values_reduced = Q_values[0, :]\n","    val = np.max(Q_values[0, :])                                                                                      #Change in dimension\n","    max_indices = np.where(Q_values[0, :] == val)[0]\n","    push_finish_button = False\n","\n","    return rd.choice(max_indices), push_finish_button\n","\n","\n","\n","\n","########################\n","## Get maximum Q_VALUE (not action)\n","#######################\n","def Q_max(state):\n","    Q_values = sess.run(Q, feed_dict={state_holder: state.reshape(1, dim_state)})\n","    return Q_values.max()\n","\n","\n","\n","\n","\n","\n","##############################\n","## ACTUAL RUN\n","###########################\n","\n","time_list = []\n","reward_list = []\n","err_list = []\n","val_list = []\n","mean_reward = 0\n","print_every_n = 50\n","trial_duration = 15\n","\n","mean_reward_list = []\n","n_episodes = trX[:, 0].size\n","for k in range(n_episodes):\n","\n","    acc_reward = 0  # Init the accumulated reward\n","    push_finish_button = False\n","    \n","    k_n = rd.randint(0, trX[:,0].size)                                            ######   !!!!!!!!!!!!!!!!!\n","\n","    if(epsilon>epsilon_min):\n","        epsilon *= epsi_decay\n","    if (learning_rate > learning_min):\n","        learning_rate *= lr_decay\n","\n","    state = copy.copy(trX[k_n, :])\n","    pos = view_size_half\n","    observation = get_field_view(state, pos, view_size_half)\n","\n","    trial_err_list = []\n","    internal_counter_memory = 0\n","    \n","    \n","    for t in range(trial_duration):  # The number of time steps in this game is maximum 200\n","        \n","\n","        old_observation = copy.copy(observation)\n","        old_internal_counter_memory = copy.copy(internal_counter_memory)\n","        state_copy = copy.copy(state)\n","\n","        #####################\n","        ## Chose action by Random or Maximum Q-value\n","        ################################\n","        if rd.rand() < epsilon:\n","          action = rd.randint(0, n_action)\n","        else:\n","          Q_values = sess.run(Q, feed_dict={state_holder: observation.reshape(1, dim_obs)})\n","          Q_values_reduced = Q_values[0, :]\n","          val = np.max(Q_values_reduced)  # Change in dimension\n","          max_indices = np.where(Q_values_reduced == val)[0]\n","          action = rd.choice(max_indices)\n","\n","\n","        ######################\n","        ## Update Environment, get reward according to chosen action\n","        #################################################\n","        reward = 0     \n","        new_state, pos, push_finish_button, reward = update_env(state_copy, pos, action,reward)\n","        new_observation = get_field_view(new_state, pos, view_size_half)\n","\n","        if(push_finish_button==False):\n","          reward += time_penalty_reward\n","        \n","        #print(\"Check train_step vars\")\n","        #print(\"observation\", observation)\n","        #print(\"new_observation\", new_observation)\n","        #print(\"action\", action_int_to_letter(action))\n","        #print(\"Reward\", reward)\n","        \n","        ############################\n","        ## TRAINING STEP, ERROR CALCULATION (RUN SESSION)\n","        ############################\n","        err = sess.run(error, feed_dict={\n","            state_holder: observation.reshape(1, dim_obs),\n","            next_state_holder: new_observation.reshape(1, dim_obs),\n","            action_holder: action,\n","            is_done_holder: np.float32(push_finish_button),\n","            r_holder: reward})\n","\n","        sess.run(training_step, feed_dict={\n","            state_holder: observation.reshape(1, dim_obs),\n","            next_state_holder: new_observation.reshape(1, dim_obs),\n","            action_holder: action,\n","            is_done_holder: np.float32(push_finish_button),\n","            r_holder: reward,\n","            learning_rate_holder: learning_rate})\n","\n","        observation = new_observation  # Pass the new state to the next step\n","        old_state = copy.copy(state)\n","        state = new_state\n","        acc_reward += reward  # Accumulate the reward\n","        trial_err_list.append(err)\n","\n","        \n","        ########################\n","        ## PRINT IN SINGLE TIME STEPS\n","        #########################\n","        \n","\n","             \n","        if(k>=100 and k<105):\n","          if(t==0):\n","            print(\"\\n ================\")\n","            print(\"K= \", k)\n","          print(\"t: \", t)\n","          print(\"Action\", action)\n","          get_animation_pic(state,pos,view_size_half,action)\n","          #print(\"observation: \", observation)\n","          #print(\"new_observation: \", new_observation)\n","          print(\"Reward: \", reward)\n","          print(\"--------------\")\n"," \n","        \n","        if (push_finish_button==True or t==trial_duration-1):\n","          mean_reward += acc_reward\n","          break\n","        \n","\n","    ########################\n","    ## PRINT MEAN AFTER EVERY N TIME STEPS\n","    ###################################\n","    if (k % print_every_n == 0):\n","        mean_reward = float(mean_reward) / print_every_n\n","        print(\"K\", k)\n","        print(k, \"th mean_reward=\", mean_reward)\n","        print(\"Epsilon: \", epsilon)\n","        mean_reward_list.append(mean_reward)\n","        mean_reward = 0.0\n","         \n","\n","    err_list.append(np.mean(trial_err_list))\n","    time_list.append(t + 1)\n","    reward_list.append(acc_reward)  # Store the result\n","\n","\n","print(\"mean_reward_list\", mean_reward_list)\n","reward_file = 'Mean_rewards/'  + str(n_h_neurons) + '_neurons_' + str(n_action) + '_max_objects'\n","np.save(reward_file, mean_reward_list)\n","\n","\n","matplotlib.get_backend()\n","legend_string = str(n_h_neurons) + \" neurons/hidden layer\"\n","plt.plot(np.arange(0,len(mean_reward_list)), mean_reward_list, label = legend_string)\n","plt.legend()\n","print(\"legend_string= \", legend_string)\n","plt.ylabel('Success rate')\n","plt.xlabel('Episodes')\n","plt.ylim(0,1)\n","title_string = 'Success rate for agent when counting binaries between 0 and ' + str(n_action) + ' binaries'\n","plt.title(title_string)\n","plt.savefig('Q_learning_binaries.png')\n","plt.show()\n"],"execution_count":83,"outputs":[{"output_type":"stream","text":["(1000, 5)\n","(1000, 1)\n","(1000, 5)\n","(1000, 1)\n","('K', 0)\n","(0, 'th mean_reward=', 0.16000000000000006)\n","('Epsilon: ', 0.995)\n","('K', 50)\n","(50, 'th mean_reward=', 4.024000000000001)\n","('Epsilon: ', 0.7744209942832988)\n","\n"," ================\n","('K= ', 100)\n","('t: ', 0)\n","('Action', 1)\n","[5. 0. 0. 1. 5.]\n"," |__I__|        STOP\n","('Reward: ', 0)\n","--------------\n","('K', 100)\n","(100, 'th mean_reward=', 6.016000000000002)\n","('Epsilon: ', 0.6027415843082742)\n","\n"," ================\n","('K= ', 101)\n","('t: ', 0)\n","('Action', 2)\n","[5. 0. 1. 1. 5.]\n"," |__I__|        L\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 1)\n","('Action', 0)\n","[5. 0. 1. 1. 5.]\n"," |__I__|        P\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 2)\n","('Action', 2)\n","[5. 0. 1. 1. 5.]\n"," |__I__|        L\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 3)\n","('Action', 0)\n","[5. 0. 1. 1. 5.]\n"," |__I__|        P\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 4)\n","('Action', 2)\n","[5. 0. 1. 1. 5.]\n"," |__I__|        L\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 5)\n","('Action', 3)\n","[5. 0. 1. 1. 5.]\n","    |__I__|     R\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 6)\n","('Action', 0)\n","[5. 0. 0. 1. 5.]\n","    |__I__|     P\n","('Reward: ', 9.8)\n","--------------\n","('t: ', 7)\n","('Action', 0)\n","[5. 0. 0. 1. 5.]\n","    |__I__|     P\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 8)\n","('Action', 0)\n","[5. 0. 0. 1. 5.]\n","    |__I__|     P\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 9)\n","('Action', 0)\n","[5. 0. 0. 1. 5.]\n","    |__I__|     P\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 10)\n","('Action', 0)\n","[5. 0. 0. 1. 5.]\n","    |__I__|     P\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 11)\n","('Action', 3)\n","[5. 0. 0. 1. 5.]\n","       |__I__|  R\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 12)\n","('Action', 0)\n","[5. 0. 0. 0. 5.]\n","       |__I__|  P\n","('Reward: ', 9.8)\n","--------------\n","('t: ', 13)\n","('Action', 3)\n","[5. 0. 0. 0. 5.]\n","       |__I__|  R\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 14)\n","('Action', 2)\n","[5. 0. 0. 0. 5.]\n","    |__I__|     L\n","('Reward: ', -0.2)\n","--------------\n","\n"," ================\n","('K= ', 102)\n","('t: ', 0)\n","('Action', 1)\n","[5. 1. 1. 1. 5.]\n"," |__I__|        STOP\n","('Reward: ', 0)\n","--------------\n","\n"," ================\n","('K= ', 103)\n","('t: ', 0)\n","('Action', 2)\n","[5. 1. 1. 1. 5.]\n"," |__I__|        L\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 1)\n","('Action', 3)\n","[5. 1. 1. 1. 5.]\n","    |__I__|     R\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 2)\n","('Action', 3)\n","[5. 1. 1. 1. 5.]\n","       |__I__|  R\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 3)\n","('Action', 2)\n","[5. 1. 1. 1. 5.]\n","    |__I__|     L\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 4)\n","('Action', 0)\n","[5. 1. 0. 1. 5.]\n","    |__I__|     P\n","('Reward: ', 9.8)\n","--------------\n","('t: ', 5)\n","('Action', 0)\n","[5. 1. 0. 1. 5.]\n","    |__I__|     P\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 6)\n","('Action', 0)\n","[5. 1. 0. 1. 5.]\n","    |__I__|     P\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 7)\n","('Action', 1)\n","[5. 1. 0. 1. 5.]\n","    |__I__|     STOP\n","('Reward: ', 0)\n","--------------\n","\n"," ================\n","('K= ', 104)\n","('t: ', 0)\n","('Action', 3)\n","[5. 1. 1. 1. 5.]\n","    |__I__|     R\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 1)\n","('Action', 3)\n","[5. 1. 1. 1. 5.]\n","       |__I__|  R\n","('Reward: ', -0.2)\n","--------------\n","('t: ', 2)\n","('Action', 1)\n","[5. 1. 1. 1. 5.]\n","       |__I__|  STOP\n","('Reward: ', 0)\n","--------------\n","('K', 150)\n","(150, 'th mean_reward=', 6.376000000000004)\n","('Epsilon: ', 0.46912134373457726)\n","('K', 200)\n","(200, 'th mean_reward=', 6.380000000000006)\n","('Epsilon: ', 0.36512303261753626)\n","('K', 250)\n","(250, 'th mean_reward=', 6.648000000000004)\n","('Epsilon: ', 0.28417984116121187)\n","('K', 300)\n","(300, 'th mean_reward=', 6.976000000000003)\n","('Epsilon: ', 0.2211807388415433)\n","('K', 350)\n","(350, 'th mean_reward=', 6.876000000000004)\n","('Epsilon: ', 0.17214774642209296)\n","('K', 400)\n","(400, 'th mean_reward=', 4.508000000000002)\n","('Epsilon: ', 0.13398475271138335)\n","('K', 450)\n","(450, 'th mean_reward=', 6.932000000000003)\n","('Epsilon: ', 0.1042820154910064)\n","('K', 500)\n","(500, 'th mean_reward=', 8.732000000000003)\n","('Epsilon: ', 0.0811640021330769)\n","('K', 550)\n","(550, 'th mean_reward=', 8.164000000000001)\n","('Epsilon: ', 0.06317096204211972)\n","('K', 600)\n","(600, 'th mean_reward=', 6.184000000000002)\n","('Epsilon: ', 0.04916675299948831)\n","('K', 650)\n","(650, 'th mean_reward=', 6.064000000000001)\n","('Epsilon: ', 0.03826710124979409)\n","('K', 700)\n","(700, 'th mean_reward=', 5.200000000000001)\n","('Epsilon: ', 0.029783765425331846)\n","('K', 750)\n","(750, 'th mean_reward=', 6.100000000000002)\n","('Epsilon: ', 0.023181078627322618)\n","('K', 800)\n","(800, 'th mean_reward=', 4.6880000000000015)\n","('Epsilon: ', 0.018042124582040707)\n","('K', 850)\n","(850, 'th mean_reward=', 4.028000000000001)\n","('Epsilon: ', 0.014042412118399107)\n","('K', 900)\n","(900, 'th mean_reward=', 4.456000000000002)\n","('Epsilon: ', 0.010929385683282892)\n","('K', 950)\n","(950, 'th mean_reward=', 3.128000000000002)\n","('Epsilon: ', 0.00998645168764533)\n","('mean_reward_list', [0.16000000000000006, 4.024000000000001, 6.016000000000002, 6.376000000000004, 6.380000000000006, 6.648000000000004, 6.976000000000003, 6.876000000000004, 4.508000000000002, 6.932000000000003, 8.732000000000003, 8.164000000000001, 6.184000000000002, 6.064000000000001, 5.200000000000001, 6.100000000000002, 4.6880000000000015, 4.028000000000001, 4.456000000000002, 3.128000000000002])\n"],"name":"stdout"},{"output_type":"error","ename":"IOError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-83-3a3b1ddc1d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean_reward_list\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0mreward_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Mean_rewards/'\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_h_neurons\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_neurons_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_max_objects'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'Mean_rewards/20_neurons_4_max_objects.npy'"]}]},{"metadata":{"id":"G40o5CrdOngY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"fdbf7318-aea5-438f-b2eb-73a731a28785","executionInfo":{"status":"ok","timestamp":1547127220263,"user_tz":-60,"elapsed":1161,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}}},"cell_type":"code","source":["a=5\n","b=a+2\n","print(\"b\",b)\n","a=7\n","print(\"b\",b)"],"execution_count":68,"outputs":[{"output_type":"stream","text":["('b', 7)\n","('b', 7)\n"],"name":"stdout"}]},{"metadata":{"id":"SsG2J09Z9-Ne","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}