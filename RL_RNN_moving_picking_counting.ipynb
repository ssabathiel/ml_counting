{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of RL_from_NN_to_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "O41vLsJc6zqs",
        "colab_type": "code",
        "outputId": "4cdbc526-3d2d-4006-84fd-8e53c2f9c2b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "cell_type": "code",
      "source": [
        "##############################\n",
        "## CREATE COUNTABLE BINARY ARRAY with field view !!!!!!\n",
        "#############################\n",
        "\n",
        "import numpy as np\n",
        "import scipy.misc as smp\n",
        "import random\n",
        "\n",
        "\n",
        "\n",
        "arr_size_half = 20\n",
        "max_N = 1\n",
        "view_size_half = 3\n",
        "object_value = 100.\n",
        "\n",
        "def create_inp_outp_array():\n",
        "    max_objects = 4\n",
        "    n_ones = random.randint(1, max_objects)\n",
        "    one_positions = random.sample(range(view_size_half, view_size_half + max_objects), n_ones)\n",
        "    binary_array = np.zeros(max_objects+2*view_size_half)\n",
        "    binary_array[one_positions] = object_value\n",
        "    #binary_array[2] = object_value\n",
        "    binary_array[0] = 9\n",
        "    binary_array[-1] = 9\n",
        "\n",
        "    return binary_array, n_ones\n",
        "    \n",
        "\n",
        " \n",
        "\n",
        "def make_data_set(n_samples):\n",
        "  mult_inp, mult_out = create_inp_outp_array()\n",
        "  for i in range(n_samples - 1):\n",
        "      single_inp, single_out = create_inp_outp_array()\n",
        "      mult_inp = np.vstack([mult_inp, single_inp])\n",
        "      mult_out = np.vstack((mult_out, single_out))\n",
        "  return mult_inp, mult_out\n",
        "\n",
        "\n",
        "\n",
        "train_input, train_output = make_data_set(10000)\n",
        "max_N=20\n",
        "test_input, test_output = make_data_set(10000)\n",
        "\n",
        "\n",
        "#Print single one\n",
        "inp_ex, out_ex = make_data_set(10)\n",
        "print(\"inp_ex\", inp_ex)\n",
        "print(\"out_ex\", out_ex)\n",
        "\n",
        "\n",
        "#Print set data\n",
        "print(\"===========================\")\n",
        "print(\"train_input: number of examples: \", train_input.shape)\n",
        "print(\"train_input: number of examples: \", train_input[:,0].size)\n",
        "print(\"train_input: single example size: \", train_input[0].size)\n",
        "print(\"train_input: dim of elements in example: \", train_input[0,0].size)\n",
        "\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "inp_ex [[  9.   0.   0. 100. 100. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0.   0.   0. 100.   0.   0.   0.   9.]\n",
            " [  9.   0.   0.   0. 100.   0.   0.   0.   0.   9.]\n",
            " [  9.   0.   0.   0.   0. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0.   0.   0. 100.   0.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100. 100. 100.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100.   0.   0.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100. 100.   0.   0.   0.   9.]\n",
            " [  9.   0.   0. 100. 100. 100. 100.   0.   0.   9.]]\n",
            "out_ex [[4]\n",
            " [1]\n",
            " [1]\n",
            " [2]\n",
            " [1]\n",
            " [4]\n",
            " [4]\n",
            " [2]\n",
            " [3]\n",
            " [4]]\n",
            "===========================\n",
            "train_input: number of examples:  (10000, 10)\n",
            "train_input: number of examples:  10000\n",
            "train_input: single example size:  10\n",
            "train_input: dim of elements in example:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bp51BbMs8GYq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########################################\n",
        "## UPDATE ENVIRONMENT WHEN MOVING, PICKING WITH LIMITED FIELD VIEW\n",
        "##################################################################\n",
        "\n",
        "\n",
        "def update_env(state, pos, action, reward):\n",
        "  \n",
        "  push_finish_button = False\n",
        "  #reward=0.0\n",
        "  if(action==0):        #[N+2] ----> pick the one at the current position\n",
        "    if(state[pos]==object_value):\n",
        "      state[pos]= 0\n",
        "      reward += point_reward\n",
        "    if(state[pos]==0):\n",
        "      reward += wrong_point_reward     \n",
        "  elif(action==1):        #[N+3] ----> stop the current counting process\n",
        "    push_finish_button = True\n",
        "  elif(action==2):          #[N] ----> go to the left\n",
        "    if(pos>view_size_half):\n",
        "      pos-=1\n",
        "  elif(action==3):        #[N+1] ----> go to the right\n",
        "     if(pos<state.size - view_size_half-1):\n",
        "      pos+=1\n",
        "  else:\n",
        "    (\"How could your action go beyond N+3?\")\n",
        "    \n",
        "  return state, pos, push_finish_button, reward \n",
        "    \n",
        "    \n",
        "def get_field_view(state, pos, view_size_half):\n",
        "  fiew_field =  copy.copy(state[pos-view_size_half:pos+view_size_half+1])\n",
        "  return fiew_field\n",
        "\n",
        "def action_int_to_letter(action):\n",
        "  \n",
        "  if(action==0):          #[N] ----> go to the left\n",
        "     act_str = \"P\"\n",
        "  elif(action==1):        #[N+1] ----> go to the Right\n",
        "     act_str = \"STOP\"\n",
        "  elif(action==2):        #[N+2] ----> pick the one at the current position\n",
        "    act_str = \"L\"\n",
        "  elif(action==3):        #[N+3] ----> stop the current counting process\n",
        "    act_str = \"R\"\n",
        "  else:\n",
        "    (\"How could your action go beyond N+3?\")\n",
        "    \n",
        "  return act_str\n",
        "    \n",
        "\n",
        "def get_animation_pic(state,pos,view_size_half,action):\n",
        "  s = \" \"\n",
        "  for i in range(pos-view_size_half):\n",
        "    s+=\"   \"\n",
        "  s += \"|\"\n",
        "  for i in range(view_size_half):\n",
        "    s+=\"__\"\n",
        "  s += \"I\"\n",
        "  for i in range(view_size_half):\n",
        "    s+=\"__\"\n",
        "  s += \"|\"\n",
        "  for i in range(state.size - (pos-view_size_half+3) ):\n",
        "    s+=\"   \"\n",
        "  s+= \"  \"\n",
        "  act_str = action_int_to_letter(action)\n",
        "  s+= act_str\n",
        "  print(state)\n",
        "  print(s)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NfZJIp7sEtOy",
        "colab_type": "code",
        "outputId": "8dc2fc7c-250a-410f-df5e-914b15ae81b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6368
        }
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "#import gym\n",
        "import numpy as np\n",
        "import numpy.random as rd\n",
        "import matplotlib.pyplot as plt\n",
        "#from gym.envs.classic_control.cartpole import CartPoleEnv\n",
        "#from cartpole_utils import plot_results,print_results\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def init_weights(shape, name):\n",
        "    return tf.Variable(tf.random_normal(shape, stddev=0.01)/ np.sqrt(dim_state), name=name)\n",
        "\n",
        "\n",
        "def Q_network(state_holder, w_h, w_h2, w_o, b, b_h):\n",
        "    h = tf.nn.relu(tf.matmul(state_holder, w_h) + b_h )\n",
        "    #h2 = tf.nn.relu(tf.matmul(h, w_h2))\n",
        "    Q =  tf.matmul(h, w_o, name='output_activation') + b \n",
        "    #Q = tf.nn.sigmoid(a_z, name='Q_model')\n",
        "    return Q\n",
        "\n",
        "\n",
        "########################\n",
        "## SET PARAMETERS\n",
        "########################\n",
        "# Algorithm parameters\n",
        "\n",
        "point_reward = +0.1\n",
        "wrong_point_reward = -0.0\n",
        "time_penalty_reward = -0.05\n",
        "final_reward = +1.0\n",
        "stop_reward = +0.0\n",
        "\n",
        "\n",
        "batch_size = 20\n",
        "learning_rate = 3e-4\n",
        "gamma = 1.0\n",
        "epsilon = 1.\n",
        "epsi_decay = .999\n",
        "lr_decay = .999\n",
        "epsilon_min = 0.01\n",
        "learning_min = 0.00001\n",
        "\n",
        "\n",
        "# Load Input=Start-of-a-game &&  Classes=Check-for-reward\n",
        "trX = train_input\n",
        "trY = train_output\n",
        "teX = test_input\n",
        "teY = test_output\n",
        "\n",
        "print(trX.shape)\n",
        "print(trY.shape)\n",
        "print(teX.shape)\n",
        "print(teY.shape)\n",
        "\n",
        "\n",
        "## SET ENVIRONMENT + #ofActions #ofStates\n",
        "dim_state = trX[0,:].size\n",
        "n_action = 4     #[Pick, Stop, Left, Right]\n",
        "dim_obs = 1+2*view_size_half\n",
        "N_print_every = 100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## BUILD TENSORFLOW\n",
        "#############################\n",
        "\n",
        "\n",
        "# PLACEHOLDER\n",
        "action_holder = tf.placeholder(dtype=tf.int32, shape=(None, 1), name=\"action_holder\")  # +1 because that is the node responsible to say if it is done ore not\n",
        "state_holder = tf.placeholder(dtype=tf.float32, shape=(None, dim_obs), name='state_holder')\n",
        "next_state_holder = tf.placeholder(dtype=tf.float32, shape=(None, dim_obs), name='next_state_holder')\n",
        "\n",
        "# Initialize the parameters of the Q model\n",
        "n_h_neurons = 20\n",
        "w_h = init_weights([dim_obs, n_h_neurons], \"w_h\")\n",
        "w_h2 = init_weights([n_h_neurons, n_h_neurons], \"w_h2\")\n",
        "w_o = init_weights([n_h_neurons, n_action], \"w_o\")\n",
        "b0 = np.zeros(n_action, dtype=np.float32)\n",
        "b_h0 = np.zeros(n_h_neurons, dtype=np.float32)\n",
        "\n",
        "b = tf.Variable(initial_value=b0, trainable=True, name='bias')\n",
        "\n",
        "b_h = tf.Variable(initial_value=b_h0, trainable=True, name='bias')\n",
        "\n",
        "Q = Q_network(state_holder, w_h, w_h2, w_o,b, b_h)\n",
        "next_Q = Q_network(next_state_holder, w_h, w_h2, w_o,b, b_h)\n",
        "\n",
        "\n",
        "### COMPARE REWARDS ####\n",
        "#PLACEHOLDER\n",
        "r_holder = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='r_holder')\n",
        "is_done_holder = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='is_done_holder')\n",
        "learning_rate_holder = tf.placeholder(dtype=tf.float32, name='symbolic_state')\n",
        "\n",
        "#### From https://lilianweng.github.io/lil-log/2018/05/05/implementing-deep-reinforcement-learning-models.html\n",
        "\n",
        "# The prediction by the primary Q network for the actual actions.\n",
        "action_one_hot = tf.one_hot(action_holder, n_action, 1.0, 0.0, name='action_one_hot')\n",
        "R = tf.reduce_sum(Q * action_one_hot, reduction_indices=-1, name='q_acted')\n",
        "\n",
        "# The optimization target defined by the Bellman equation and the target network.\n",
        "max_q_next_by_target = tf.reduce_max(next_Q, axis=-1)\n",
        "R_next = r_holder + (1. - is_done_holder) * gamma * max_q_next_by_target\n",
        "\n",
        "# The loss measures the mean squared error between prediction and target.\n",
        "err = tf.reduce_mean(tf.square(R - tf.stop_gradient(R_next)), name=\"loss_mse_train\")\n",
        "training_step = tf.train.AdamOptimizer(learning_rate=learning_rate_holder).minimize(err, name=\"adam_optim\")\n",
        "\n",
        "###########################################################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sess = tf.Session()  # FOR NOW everything is symbolic, this object has to be called to compute each value of Q\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "################################\n",
        "################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################################\n",
        "## PI(S)\n",
        "## First check if its random time THEN\n",
        "        # - calculate State--Network--> Q\n",
        "        # - take maximum of Q\n",
        "        # - just remember and return index of maximum    ---> returns ACTION that leads to max_Q-value (not value itself)\n",
        "################################\n",
        "\n",
        "def policy(state):\n",
        "    if rd.rand() < epsilon:\n",
        "        return rd.randint(0, n_action)\n",
        "    reshaped_state = state.reshape(1, dim_state)\n",
        "    Q_values = sess.run(Q, feed_dict={state_holder: state.reshape(1, dim_state)})\n",
        "    Q_values_reduced = Q_values[0, :]\n",
        "    val = np.max(Q_values[0, :])                                                                                      #Change in dimension\n",
        "    max_indices = np.where(Q_values[0, :] == val)[0]\n",
        "    push_finish_button = False\n",
        "\n",
        "    return rd.choice(max_indices), push_finish_button\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################\n",
        "## Get maximum Q_VALUE (not action)\n",
        "#######################\n",
        "def Q_max(state):\n",
        "    Q_values = sess.run(Q, feed_dict={state_holder: state.reshape(1, dim_state)})\n",
        "    return Q_values.max()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##############################\n",
        "## ACTUAL RUN\n",
        "###########################\n",
        "\n",
        "time_list = []\n",
        "reward_list = []\n",
        "err_list = []\n",
        "val_list = []\n",
        "mean_reward = 0\n",
        "print_every_n = 50\n",
        "trial_duration = 10\n",
        "\n",
        "mean_reward_list = []\n",
        "n_episodes = trX[:, 0].size\n",
        "\n",
        "class ExperienceBuffer:\n",
        "  observation = np.array([]).reshape(0,dim_obs)\n",
        "  new_observation = np.array([]).reshape(0,dim_obs)\n",
        "  action = np.empty(0)\n",
        "  push_finish_button = np.empty(0)\n",
        "  reward = np.empty(0)\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "buffer_size = 40\n",
        "\n",
        "\n",
        "for k in range(n_episodes):\n",
        "\n",
        "    acc_reward = 0  # Init the accumulated reward\n",
        "    push_finish_button = False\n",
        "    \n",
        "    k_n = rd.randint(0, trX[:,0].size)                                            ######   !!!!!!!!!!!!!!!!!\n",
        "\n",
        "\n",
        "    state = copy.copy(trX[k_n, :])\n",
        "    pos = view_size_half\n",
        "    observation = get_field_view(state, pos, view_size_half)\n",
        "\n",
        "    trial_err_list = []\n",
        "    internal_counter_memory = 0\n",
        "    \n",
        "    \n",
        "    for t in range(trial_duration):  # The number of time steps in this game is maximum 200\n",
        "        \n",
        "\n",
        "        old_observation = copy.copy(observation)\n",
        "        old_internal_counter_memory = copy.copy(internal_counter_memory)\n",
        "        state_copy = copy.copy(state)\n",
        "\n",
        "        #####################\n",
        "        ## Chose action by Random or Maximum Q-value\n",
        "        ################################\n",
        "        \n",
        "        Q_values = sess.run(Q, feed_dict={state_holder: observation.reshape(1, dim_obs)} )\n",
        "        Q_values_reduced = Q_values[0, :]\n",
        "        val = np.max(Q_values_reduced)  # Change in dimension\n",
        "        max_indices = np.where(Q_values_reduced == val)[0]\n",
        "        \n",
        "        if rd.rand() < epsilon:\n",
        "          action = rd.randint(0, n_action)\n",
        "        else:\n",
        "\n",
        "          action = rd.choice(max_indices)\n",
        "\n",
        "\n",
        "        ######################\n",
        "        ## Update Environment, get reward according to chosen action\n",
        "        #################################################\n",
        "        reward = 0     \n",
        "        new_state, pos, push_finish_button, reward = update_env(state_copy, pos, action,reward)\n",
        "        new_observation = get_field_view(new_state, pos, view_size_half)\n",
        "\n",
        "        if(push_finish_button==False):\n",
        "          reward += time_penalty_reward\n",
        "        \n",
        "        if (push_finish_button==True or t==trial_duration-1):\n",
        "          if(new_state[1]==0 and new_state[2]==0):\n",
        "            reward += final_reward\n",
        "        if(new_state[1]==0 and new_state[2]==0):\n",
        "            reward += final_reward\n",
        "            push_finish_button = True\n",
        "\n",
        "        \n",
        "        ############################\n",
        "        ## TRAINING STEP, ERROR CALCULATION (RUN SESSION)\n",
        "        ############################\n",
        "        \n",
        "\n",
        "        if(len(exp_buffer.observation) < batch_size):  \n",
        "            \n",
        "            exp_buffer.observation = np.vstack([exp_buffer.observation, observation])\n",
        "            exp_buffer.new_observation = np.vstack([exp_buffer.new_observation, new_observation])\n",
        "            exp_buffer.action = np.append(exp_buffer.action, action)\n",
        "            exp_buffer.reward = np.append(exp_buffer.reward, reward)\n",
        "            exp_buffer.push_finish_button = np.append(exp_buffer.push_finish_button, np.float32(push_finish_button))\n",
        "            \n",
        "        else:\n",
        "            rand_i = random.randint(0,batch_size-1)\n",
        "            exp_buffer.observation[rand_i] = observation\n",
        "            exp_buffer.new_observation[rand_i] = new_observation\n",
        "            exp_buffer.action[rand_i] = action\n",
        "            exp_buffer.reward[rand_i] = reward\n",
        "            exp_buffer.push_finish_button[rand_i] = np.float32(push_finish_button)\n",
        "        \n",
        "        buffer_size = exp_buffer.observation[:,0].size\n",
        "        \n",
        "        if(buffer_size >= batch_size):\n",
        "          \n",
        "          \n",
        "          indy = 0 #random.sample(range(buffer_size), batch_size)\n",
        "     \n",
        "          error, R_out, R_next_out = sess.run([err, R, R_next], feed_dict={\n",
        "              state_holder: exp_buffer.observation.reshape(batch_size, dim_obs),\n",
        "              next_state_holder: exp_buffer.new_observation.reshape(batch_size, dim_obs) ,\n",
        "              action_holder: exp_buffer.action.reshape(batch_size, 1),\n",
        "              is_done_holder: exp_buffer.push_finish_button.reshape(batch_size, 1),\n",
        "              r_holder: exp_buffer.reward.reshape(batch_size, 1)})   \n",
        "\n",
        "          sess.run(training_step, feed_dict={\n",
        "              state_holder: exp_buffer.observation.reshape(batch_size, dim_obs),\n",
        "              next_state_holder: exp_buffer.new_observation.reshape(batch_size, dim_obs),\n",
        "              action_holder: exp_buffer.action.reshape(batch_size, 1),\n",
        "              is_done_holder: exp_buffer.push_finish_button.reshape(batch_size, 1),\n",
        "              r_holder: exp_buffer.reward.reshape(batch_size, 1),\n",
        "              learning_rate_holder: learning_rate})\n",
        "          \n",
        "          if(epsilon>epsilon_min):\n",
        "             epsilon *= epsi_decay\n",
        "          if (learning_rate > learning_min):\n",
        "             learning_rate *= lr_decay\n",
        "        \n",
        "        Q_values_after_train = sess.run(Q, feed_dict={state_holder: observation.reshape(1, dim_obs)} )\n",
        "        observation = new_observation  # Pass the new state to the next step\n",
        "        old_state = copy.copy(state)\n",
        "        state = new_state\n",
        "        acc_reward += reward  # Accumulate the reward\n",
        "        #trial_err_list.append(error)\n",
        "\n",
        "        \n",
        "        ########################\n",
        "        ## PRINT IN SINGLE TIME STEPS\n",
        "        #########################\n",
        "        \n",
        "\n",
        "             \n",
        "        if( (k>=0 and k<9) and (k>=700 and k<709) ):\n",
        "        #if(mean_reward_list[-1]>0.8):\n",
        "          \n",
        "          if(t==0):\n",
        "            print(\"\\n ================\")\n",
        "            print(\"K= \", k)\n",
        "          print(\"t: \", t)\n",
        "          print(\"Action\", action)\n",
        "          get_animation_pic(state,pos,view_size_half,action)\n",
        "          print(\"observation 2: \", observation)\n",
        "          #print(\"new_observation: \", new_observation)\n",
        "          print(\"Q_values:             \", Q_values)\n",
        "          print(\"Q_values_after_train: \", Q_values_after_train)\n",
        "          print(\"For Observation 3: \", old_observation)\n",
        "          print(\"R \", R_out)\n",
        "          print(\"R_next \", R_next_out)\n",
        "          print(\"exp_buffer.reward[indy].reshape(batch_size, 1) \", exp_buffer.reward[indy].reshape(batch_size, 1))\n",
        "          \n",
        "          \n",
        "          print(\"Reward: \", reward)\n",
        "          print(\"--------------\")\n",
        " \n",
        "        \n",
        "        if (push_finish_button==True or t==trial_duration-1):\n",
        "          mean_reward += acc_reward\n",
        "          break\n",
        "        \n",
        "\n",
        "    ########################\n",
        "    ## PRINT MEAN AFTER EVERY N TIME STEPS\n",
        "    ###################################\n",
        "    if (k % print_every_n == 0):\n",
        "        mean_reward = float(mean_reward) / print_every_n\n",
        "        print(\"K\", k)\n",
        "        print(k, \"th mean_reward=\", mean_reward)\n",
        "        print(\"Epsilon: \", epsilon)\n",
        "        mean_reward_list.append(mean_reward)\n",
        "        mean_reward = 0.0\n",
        "         \n",
        "\n",
        "    #err_list.append(np.mean(trial_err_list))\n",
        "    time_list.append(t + 1)\n",
        "    reward_list.append(acc_reward)  # Store the result\n",
        "\n",
        "\n",
        "print(\"mean_reward_list\", mean_reward_list)\n",
        "reward_file = 'Mean_rewards/'  + str(n_h_neurons) + '_neurons_' + str(n_action) + '_max_objects'\n",
        "np.save(reward_file, mean_reward_list)\n",
        "\n",
        "\n",
        "matplotlib.get_backend()\n",
        "legend_string = str(n_h_neurons) + \" neurons/hidden layer\"\n",
        "plt.plot(np.arange(0,len(mean_reward_list)), mean_reward_list, label = legend_string)\n",
        "plt.legend()\n",
        "print(\"legend_string= \", legend_string)\n",
        "plt.ylabel('Success rate')\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylim(0,1)\n",
        "title_string = 'Success rate for agent when counting binaries between 0 and ' + str(n_action) + ' binaries'\n",
        "plt.title(title_string)\n",
        "plt.savefig('Q_learning_binaries.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 10)\n",
            "(10000, 1)\n",
            "(10000, 10)\n",
            "(10000, 1)\n",
            "K 0\n",
            "0 th mean_reward= 0.04\n",
            "Epsilon:  1.0\n",
            "K 50\n",
            "50 th mean_reward= 1.2370000000000003\n",
            "Epsilon:  0.9684910757595269\n",
            "K 100\n",
            "100 th mean_reward= 1.2810000000000008\n",
            "Epsilon:  0.9212341621210596\n",
            "K 150\n",
            "150 th mean_reward= 1.2830000000000001\n",
            "Epsilon:  0.8762831198969288\n",
            "K 200\n",
            "200 th mean_reward= 1.1550000000000005\n",
            "Epsilon:  0.8335254355400126\n",
            "K 250\n",
            "250 th mean_reward= 1.3610000000000009\n",
            "Epsilon:  0.7928540855310416\n",
            "K 300\n",
            "300 th mean_reward= 1.2750000000000006\n",
            "Epsilon:  0.7541672684961374\n",
            "K 350\n",
            "350 th mean_reward= 1.3990000000000002\n",
            "Epsilon:  0.7173681503955072\n",
            "K 400\n",
            "400 th mean_reward= 1.3780000000000008\n",
            "Epsilon:  0.6823646221455009\n",
            "K 450\n",
            "450 th mean_reward= 1.4070000000000003\n",
            "Epsilon:  0.649069069067341\n",
            "K 500\n",
            "500 th mean_reward= 1.4810000000000008\n",
            "Epsilon:  0.6173981515854621\n",
            "K 550\n",
            "550 th mean_reward= 1.5020000000000007\n",
            "Epsilon:  0.5872725966265356\n",
            "K 600\n",
            "600 th mean_reward= 1.497\n",
            "Epsilon:  0.5586169991970452\n",
            "K 650\n",
            "650 th mean_reward= 1.4490000000000003\n",
            "Epsilon:  0.5313596336427661\n",
            "K 700\n",
            "700 th mean_reward= 1.5710000000000002\n",
            "Epsilon:  0.505432274117712\n",
            "K 750\n",
            "750 th mean_reward= 1.5860000000000003\n",
            "Epsilon:  0.48077002381319206\n",
            "K 800\n",
            "800 th mean_reward= 1.6740000000000004\n",
            "Epsilon:  0.4573111525195289\n",
            "K 850\n",
            "850 th mean_reward= 1.6720000000000002\n",
            "Epsilon:  0.43499694211384704\n",
            "K 900\n",
            "900 th mean_reward= 1.7100000000000002\n",
            "Epsilon:  0.41377153958718943\n",
            "K 950\n",
            "950 th mean_reward= 1.6890000000000003\n",
            "Epsilon:  0.3935818172430853\n",
            "K 1000\n",
            "1000 th mean_reward= 1.6740000000000002\n",
            "Epsilon:  0.37437723971763814\n",
            "K 1050\n",
            "1050 th mean_reward= 1.7080000000000004\n",
            "Epsilon:  0.35610973748828684\n",
            "K 1100\n",
            "1100 th mean_reward= 1.7140000000000004\n",
            "Epsilon:  0.3387335865546259\n",
            "K 1150\n",
            "1150 th mean_reward= 1.7520000000000004\n",
            "Epsilon:  0.3222052939901265\n",
            "K 1200\n",
            "1200 th mean_reward= 1.6910000000000005\n",
            "Epsilon:  0.3064834890782873\n",
            "K 1250\n",
            "1250 th mean_reward= 1.794\n",
            "Epsilon:  0.29152881976073036\n",
            "K 1300\n",
            "1300 th mean_reward= 1.8570000000000002\n",
            "Epsilon:  0.27730385413804465\n",
            "K 1350\n",
            "1350 th mean_reward= 1.7730000000000001\n",
            "Epsilon:  0.2637729867768368\n",
            "K 1400\n",
            "1400 th mean_reward= 1.8530000000000002\n",
            "Epsilon:  0.2509023495884683\n",
            "K 1450\n",
            "1450 th mean_reward= 1.7920000000000003\n",
            "Epsilon:  0.2386597270564102\n",
            "K 1500\n",
            "1500 th mean_reward= 1.697\n",
            "Epsilon:  0.22701447560002463\n",
            "K 1550\n",
            "1550 th mean_reward= 1.7690000000000003\n",
            "Epsilon:  0.21593744687294092\n",
            "K 1600\n",
            "1600 th mean_reward= 1.9160000000000001\n",
            "Epsilon:  0.20540091480403888\n",
            "K 1650\n",
            "1650 th mean_reward= 1.7920000000000003\n",
            "Epsilon:  0.19537850619842073\n",
            "K 1700\n",
            "1700 th mean_reward= 1.92\n",
            "Epsilon:  0.18584513472466632\n",
            "K 1750\n",
            "1750 th mean_reward= 1.878\n",
            "Epsilon:  0.17677693812313813\n",
            "K 1800\n",
            "1800 th mean_reward= 1.815\n",
            "Epsilon:  0.16815121847816739\n",
            "K 1850\n",
            "1850 th mean_reward= 1.834\n",
            "Epsilon:  0.15994638540462147\n",
            "K 1900\n",
            "1900 th mean_reward= 1.8969999999999998\n",
            "Epsilon:  0.15214190200664743\n",
            "K 1950\n",
            "1950 th mean_reward= 1.8360000000000003\n",
            "Epsilon:  0.1447182334733242\n",
            "K 2000\n",
            "2000 th mean_reward= 1.8780000000000001\n",
            "Epsilon:  0.1376567981825579\n",
            "K 2050\n",
            "2050 th mean_reward= 1.9180000000000001\n",
            "Epsilon:  0.13093992119083192\n",
            "K 2100\n",
            "2100 th mean_reward= 1.798\n",
            "Epsilon:  0.12455078999239512\n",
            "K 2150\n",
            "2150 th mean_reward= 1.8970000000000002\n",
            "Epsilon:  0.11847341243715274\n",
            "K 2200\n",
            "2200 th mean_reward= 1.9160000000000001\n",
            "Epsilon:  0.11269257670192787\n",
            "K 2250\n",
            "2250 th mean_reward= 1.9160000000000001\n",
            "Epsilon:  0.1071938132148994\n",
            "K 2300\n",
            "2300 th mean_reward= 1.895\n",
            "Epsilon:  0.1019633584379136\n",
            "K 2350\n",
            "2350 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.09698812041601469\n",
            "K 2400\n",
            "2400 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.09225564600796458\n",
            "K 2450\n",
            "2450 th mean_reward= 1.92\n",
            "Epsilon:  0.08775408971572891\n",
            "K 2500\n",
            "2500 th mean_reward= 1.901\n",
            "Epsilon:  0.08347218403490854\n",
            "K 2550\n",
            "2550 th mean_reward= 1.979\n",
            "Epsilon:  0.07939921125190336\n",
            "K 2600\n",
            "2600 th mean_reward= 1.96\n",
            "Epsilon:  0.07552497661721554\n",
            "K 2650\n",
            "2650 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.07183978282774463\n",
            "K 2700\n",
            "2700 th mean_reward= 1.922\n",
            "Epsilon:  0.06833440575420312\n",
            "K 2750\n",
            "2750 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.06500007135289768\n",
            "K 2800\n",
            "2800 th mean_reward= 1.939\n",
            "Epsilon:  0.06182843370408491\n",
            "K 2850\n",
            "2850 th mean_reward= 1.9160000000000001\n",
            "Epsilon:  0.0588115541219326\n",
            "K 2900\n",
            "2900 th mean_reward= 1.96\n",
            "Epsilon:  0.055941881283796624\n",
            "K 2950\n",
            "2950 th mean_reward= 1.8970000000000002\n",
            "Epsilon:  0.053212232329077506\n",
            "K 3000\n",
            "3000 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.050615774880346555\n",
            "K 3050\n",
            "3050 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.048146009941739586\n",
            "K 3100\n",
            "3100 th mean_reward= 1.96\n",
            "Epsilon:  0.045796755631812916\n",
            "K 3150\n",
            "3150 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.04356213171014454\n",
            "K 3200\n",
            "3200 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.041436544858949854\n",
            "K 3250\n",
            "3250 th mean_reward= 1.979\n",
            "Epsilon:  0.03941467468287187\n",
            "K 3300\n",
            "3300 th mean_reward= 1.979\n",
            "Epsilon:  0.03749146039190275\n",
            "K 3350\n",
            "3350 th mean_reward= 1.9809999999999999\n",
            "Epsilon:  0.03566208813410397\n",
            "K 3400\n",
            "3400 th mean_reward= 1.9160000000000001\n",
            "Epsilon:  0.03392197894641826\n",
            "K 3450\n",
            "3450 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.03226677729341426\n",
            "K 3500\n",
            "3500 th mean_reward= 2.0\n",
            "Epsilon:  0.030692340165275823\n",
            "K 3550\n",
            "3550 th mean_reward= 2.0\n",
            "Epsilon:  0.02919472670774816\n",
            "K 3600\n",
            "3600 th mean_reward= 2.0\n",
            "Epsilon:  0.02777018835808423\n",
            "K 3650\n",
            "3650 th mean_reward= 1.979\n",
            "Epsilon:  0.026415159462301403\n",
            "K 3700\n",
            "3700 th mean_reward= 1.979\n",
            "Epsilon:  0.025126248350263173\n",
            "K 3750\n",
            "3750 th mean_reward= 1.979\n",
            "Epsilon:  0.023900228846246697\n",
            "K 3800\n",
            "3800 th mean_reward= 2.0\n",
            "Epsilon:  0.0227340321937469\n",
            "K 3850\n",
            "3850 th mean_reward= 2.0\n",
            "Epsilon:  0.021624739374304553\n",
            "K 3900\n",
            "3900 th mean_reward= 1.979\n",
            "Epsilon:  0.020569573801132425\n",
            "K 3950\n",
            "3950 th mean_reward= 1.979\n",
            "Epsilon:  0.01956589436925137\n",
            "K 4000\n",
            "4000 th mean_reward= 2.0\n",
            "Epsilon:  0.01861118884474051\n",
            "K 4050\n",
            "4050 th mean_reward= 2.0\n",
            "Epsilon:  0.017703067576554966\n",
            "K 4100\n",
            "4100 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.01683925751517146\n",
            "K 4150\n",
            "4150 th mean_reward= 2.0\n",
            "Epsilon:  0.01601759652309025\n",
            "K 4200\n",
            "4200 th mean_reward= 2.0\n",
            "Epsilon:  0.015236027962952649\n",
            "K 4250\n",
            "4250 th mean_reward= 1.96\n",
            "Epsilon:  0.014492595549727911\n",
            "K 4300\n",
            "4300 th mean_reward= 2.0\n",
            "Epsilon:  0.013785438454084442\n",
            "K 4350\n",
            "4350 th mean_reward= 2.0\n",
            "Epsilon:  0.013112786644688904\n",
            "K 4400\n",
            "4400 th mean_reward= 2.0\n",
            "Epsilon:  0.012472956457774959\n",
            "K 4450\n",
            "4450 th mean_reward= 2.0\n",
            "Epsilon:  0.011864346382892048\n",
            "K 4500\n",
            "4500 th mean_reward= 2.0\n",
            "Epsilon:  0.011285433054286009\n",
            "K 4550\n",
            "4550 th mean_reward= 2.0\n",
            "Epsilon:  0.010734767437877667\n",
            "K 4600\n",
            "4600 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.010210971204295469\n",
            "K 4650\n",
            "4650 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4700\n",
            "4700 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4750\n",
            "4750 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4800\n",
            "4800 th mean_reward= 1.9580000000000002\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4850\n",
            "4850 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4900\n",
            "4900 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n",
            "K 4950\n",
            "4950 th mean_reward= 1.979\n",
            "Epsilon:  0.009998671593271896\n",
            "K 5000\n",
            "5000 th mean_reward= 1.979\n",
            "Epsilon:  0.009998671593271896\n",
            "K 5050\n",
            "5050 th mean_reward= 1.9370000000000003\n",
            "Epsilon:  0.009998671593271896\n",
            "K 5100\n",
            "5100 th mean_reward= 1.979\n",
            "Epsilon:  0.009998671593271896\n",
            "K 5150\n",
            "5150 th mean_reward= 1.979\n",
            "Epsilon:  0.009998671593271896\n",
            "K 5200\n",
            "5200 th mean_reward= 2.0\n",
            "Epsilon:  0.009998671593271896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-0e517de601eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    287\u001b[0m               \u001b[0maction_holder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexp_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m               \u001b[0mis_done_holder\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexp_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_finish_button\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m               r_holder: exp_buffer.reward.reshape(batch_size, 1)})   \n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m           sess.run(training_step, feed_dict={\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1137\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mdirect\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \"\"\"\n\u001b[0;32m--> 470\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_controller\u001b[0;34m(self, default)\u001b[0m\n\u001b[1;32m   5226\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5227\u001b[0m       with super(_DefaultGraphStack, self).get_controller(\n\u001b[0;32m-> 5228\u001b[0;31m           default) as g, context.graph_mode():\n\u001b[0m\u001b[1;32m   5229\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5230\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36mhelper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \"\"\"\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhelper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_GeneratorContextManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "G40o5CrdOngY",
        "colab_type": "code",
        "outputId": "fdbf7318-aea5-438f-b2eb-73a731a28785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "a=5\n",
        "b=a+2\n",
        "print(\"b\",b)\n",
        "a=7\n",
        "print(\"b\",b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('b', 7)\n",
            "('b', 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SsG2J09Z9-Ne",
        "colab_type": "code",
        "outputId": "2c16bf5a-dcf7-4c8a-fa02-3fbbab9d44ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  observation = []\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "exp_buffer.observation.append(5)\n",
        "exp_buffer.observation.append(7)\n",
        "\n",
        "print(exp_buffer.observation)\n",
        "print(\"Length\", len(exp_buffer.observation))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5, 7]\n",
            "('Length', 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pzIFA6iBiAb7",
        "colab_type": "code",
        "outputId": "8dc9eff1-d0be-405e-e69a-1643cbf245fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  observation = np.empty(0)\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,5)\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,7)\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,9)\n",
        "exp_buffer.observation = np.append(exp_buffer.observation,1)\n",
        "\n",
        "print(exp_buffer.observation)\n",
        "print(\"Length\", exp_buffer.observation.size)\n",
        "\n",
        "\n",
        "randy_arr = np.random.choice(exp_buffer.observation,2)\n",
        "print(\"randy_arr= \", randy_arr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5. 7. 9. 1.]\n",
            "('Length', 4)\n",
            "('randy_arr= ', array([1., 7.]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IHIo8MDYorVp",
        "colab_type": "code",
        "outputId": "3fff3946-6e04-4004-87fe-ba4cf03ef784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "class ExperienceBuffer:\n",
        "  observation = np.array([]).reshape(0,3)\n",
        "  \n",
        "exp_buffer = ExperienceBuffer()\n",
        "\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [1,2,3]])\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [9,9,9]])\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [4,4,4]])\n",
        "exp_buffer.observation = np.vstack([exp_buffer.observation, [1,1,1]])\n",
        "buffer_size = exp_buffer.observation[:,0].size\n",
        "\n",
        "print(exp_buffer.observation)\n",
        "print(\"Length\", buffer_size)\n",
        "\n",
        "batch_size = 2\n",
        "\n",
        "#randy_arr = np.random.choice(exp_buffer.observation,2)\n",
        "#print(\"randy_arr= \", randy_arr)\n",
        "\n",
        "#indy = [0,2]\n",
        "indy = random.sample(range(buffer_size), batch_size)\n",
        "print(\"selected vectors: \", exp_buffer.observation[indy])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1. 2. 3.]\n",
            " [9. 9. 9.]\n",
            " [4. 4. 4.]\n",
            " [1. 1. 1.]]\n",
            "('Length', 4)\n",
            "('selected vectors: ', array([[4., 4., 4.],\n",
            "       [1., 2., 3.]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rzQvHWgqEX00",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ffb1693-495e-4533-8f2b-23040c1bfffb"
      },
      "cell_type": "code",
      "source": [
        "a = np.empty(0)\n",
        "a = np.append(a, 1)\n",
        "b=a.reshape(batch_size, 1)\n",
        "print(\"b: \", b)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b:  [[1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YJxzKWWIX_iV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "16e9f61d-ffdf-4a16-fc85-d8bbe0fa8048"
      },
      "cell_type": "code",
      "source": [
        "#a = random.sample(range(1), 1)\n",
        "a= range(1)\n",
        "b= random.sample(a, 1)\n",
        "print(\"a= \", a )\n",
        "print(\"b= \", b )"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a=  range(0, 1)\n",
            "b=  [0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FIeM__jYYlcX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}