{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RL_from_NN_to_RNN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"kFQTg4GfEpFP","colab_type":"code","outputId":"15f63aaa-6f96-4d71-fd1f-c69644ec2479","executionInfo":{"status":"ok","timestamp":1547114763741,"user_tz":-60,"elapsed":669,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}},"colab":{"base_uri":"https://localhost:8080/","height":782}},"cell_type":"code","source":["##############################\n","## CREATE COUNTABLE BINARY ARRAY\n","#############################\n","\n","import numpy as np\n","import scipy.misc as smp\n","import random\n","\n","\n","\n","arr_size_half = 20\n","max_N = 3\n","view_size_half = 0\n","\n","def create_inp_outp_array():\n","    max_objects = 1\n","    n_ones = random.randint(1, max_objects)\n","    one_positions = random.sample(range(view_size_half, view_size_half + max_objects), n_ones)\n","    binary_array = np.zeros(max_objects+2*view_size_half)\n","    binary_array[one_positions] = 255\n","    return binary_array, n_ones\n","    \n","\n"," \n","\n","def make_data_set(n_samples):\n","  mult_inp, mult_out = create_inp_outp_array()\n","  for i in range(n_samples - 1):\n","      single_inp, single_out = create_inp_outp_array()\n","      mult_inp = np.vstack([mult_inp, single_inp])\n","      mult_out = np.vstack((mult_out, single_out))\n","  return mult_inp, mult_out\n","\n","\n","\n","train_input, train_output = make_data_set(1000)\n","max_N=20\n","test_input, test_output = make_data_set(1000)\n","\n","\n","#Print single one\n","inp_ex, out_ex = make_data_set(20)\n","print(\"inp_ex\", inp_ex)\n","print(\"out_ex\", out_ex)\n","\n","\n","#Print set data\n","print(\"===========================\")\n","print(\"train_input: number of examples: \", train_input.shape)\n","print(\"train_input: number of examples: \", train_input[:,0].size)\n","print(\"train_input: single example size: \", train_input[0].size)\n","print(\"train_input: dim of elements in example: \", train_input[0,0].size)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["('inp_ex', array([[255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.],\n","       [255.]]))\n","('out_ex', array([[1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1],\n","       [1]]))\n","===========================\n","('train_input: number of examples: ', (1000, 1))\n","('train_input: number of examples: ', 1000)\n","('train_input: single example size: ', 1)\n","('train_input: dim of elements in example: ', 1)\n"],"name":"stdout"}]},{"metadata":{"id":"NfZJIp7sEtOy","colab_type":"code","outputId":"17eba65f-245b-4fe0-d9f9-fd9b5046a8a2","executionInfo":{"status":"error","timestamp":1547129118726,"user_tz":-60,"elapsed":9116,"user":{"displayName":"Sil Sabat","photoUrl":"","userId":"17135623927395442554"}},"colab":{"base_uri":"https://localhost:8080/","height":2067}},"cell_type":"code","source":["#!/usr/bin/env python\n","\n","import numpy as np\n","import numpy.random as rd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import copy\n","import matplotlib\n","import matplotlib.pyplot as plt\n","\n","\n","def init_weights(shape, name):\n","    return tf.Variable(tf.random_normal(shape, stddev=0.01)/ np.sqrt(dim_state), name=name)\n","\n","# This network is the same as the previous one except with an extra hidden layer + dropout\n","def Q_network(state_holder, w_h, w_h2, w_o, b):\n","    h = tf.nn.relu(tf.matmul(state_holder, w_h))\n","    h2 = tf.nn.relu(tf.matmul(h, w_h2))\n","    Q = tf.matmul(h2, w_o, name='output_activation') + b\n","    #Q = tf.nn.sigmoid(a_z, name='Q_model')\n","    return Q\n","\n","\n","########################\n","## SET PARAMETERS\n","########################\n","# Algorithm parameters\n","\n","step_reward = +0.0\n","final_reward = +0.0\n","time_penalty_reward = -0.2\n","\n","learning_rate = 3e-4\n","gamma = 1.0\n","epsilon = 1.\n","epsi_decay = .99\n","lr_decay = .999\n","epsilon_min = 0.01\n","learning_min = 0.00001\n","\n","\n","# Load Input=Start-of-a-game &&  Classes=Check-for-reward\n","trX = train_input\n","trY = train_output\n","teX = test_input\n","teY = test_output\n","\n","print(trX.shape)\n","print(trY.shape)\n","print(teX.shape)\n","print(teY.shape)\n","\n","\n","## SET ENVIRONMENT + #ofActions #ofStates\n","dim_state = trX[0,:].size\n","n_action = trX[0,:].size\n","\n","\n","\n","# General parameters\n","render = False\n","N_print_every = 100\n","N_trial = trX[:,0].size\n","N_trial_test = N_trial = teX[:,0].size\n","\n","\n","\n","\n","\n","\n","\n","\n","##############################\n","## BUILD NETWORK\n","#############################\n","\n","\n","\n","# PLACEHOLDER\n","action_holder = tf.placeholder(dtype=tf.int32, name=\"Y\")  # +1 because that is the node responsible to say if it is done ore not\n","state_holder = tf.placeholder(dtype=tf.float32, shape=(1, dim_state), name='symbolic_state')\n","next_state_holder = tf.placeholder(dtype=tf.float32, shape=(1, dim_state), name='symbolic_state')\n","\n","# Initialize the parameters of the Q model\n","n_h_neurons = 600\n","w_h = init_weights([trX[0,:].size, n_h_neurons], \"w_h\")\n","w_h2 = init_weights([n_h_neurons, n_h_neurons], \"w_h2\")\n","w_o = init_weights([n_h_neurons, n_action+1], \"w_o\")\n","b0 = np.zeros(n_action+1 , dtype=np.float32)\n","b = tf.Variable(initial_value=b0, trainable=True, name='bias')\n","\n","Q = Q_network(state_holder, w_h, w_h2, w_o,b)\n","next_Q = Q_network(next_state_holder, w_h, w_h2, w_o,b)\n","\n","\n","\n","### COMPARE REWARDS ####\n","#PLACEHOLDER\n","r_holder = tf.placeholder(dtype=tf.float32, name='symbolic_value_estimation')\n","is_done_holder = tf.placeholder(dtype=tf.float32, name='is_done')\n","\n","next_action_Q_max = tf.to_int32(tf.argmax(next_Q, dimension=1, name='next_step_maxQ_action'))           # [ 0.1   0.4   0.8  0.8] -->  3, 4\n","# next_action_holder = tf.placeholder(dtype=tf.int32, name='symbolic_next_action')\n","R = Q[0, action_holder]\n","next_R = r_holder + gamma * next_Q[0, next_action_Q_max[0]] * (1 - is_done_holder)\n","error = (R - next_R)**2\n","\n","# Define the operation that performs the optimization\n","learning_rate_holder = tf.placeholder(dtype=tf.float32, name='symbolic_state')\n","training_step = tf.train.GradientDescentOptimizer(learning_rate_holder).minimize(error)\n","\n","sess = tf.Session()  # FOR NOW everything is symbolic, this object has to be called to compute each value of Q\n","sess.run(tf.initialize_all_variables())\n","\n","################################\n","################################\n","\n","\n","\n","\n","\n","################################\n","## PI(S)\n","## First check if its random time THEN\n","        # - calculate State--Network--> Q\n","        # - take maximum of Q\n","        # - just remember and return index of maximum    ---> returns ACTION that leads to max_Q-value (not value itself)\n","################################\n","\n","def policy(state):\n","    if rd.rand() < epsilon:\n","        return rd.randint(0, n_action)\n","    reshaped_state = state.reshape(1, dim_state)\n","    Q_values = sess.run(Q, feed_dict={state_holder: state.reshape(1, dim_state)})\n","    Q_values_reduced = Q_values[0, :]\n","    val = np.max(Q_values[0, :])                                                                                      #Change in dimension\n","    max_indices = np.where(Q_values[0, :] == val)[0]\n","    push_finish_button = False\n","    #print(\"Q_values[0,-1]\", Q_values[0,-1])\n","    #if(Q_values[0,-1]>0.0):\n","        #push_finish_button = True\n","\n","    return rd.choice(max_indices), push_finish_button\n","\n","\n","# define Action\n","def draw_point(state, action, finish_button,internal_counter_memory):    #later for convolutoinal NN --> state = matrix rank-2 like a real image is\n","    reward = 0\n","    state_copy = copy.copy(state)\n","    #print(\"draw point\")\n","    #if(finish_button==False):\n","        #print(\"finish_button = False\")\n","    if(finish_button==False):\n","      if(state[action]==255):\n","          #print(\"state[action]==1\")\n","          #pass\n","          #print(\"drew on white\")\n","          reward += step_reward                      #Intermediate reward: giving points if set point on white area\n","          internal_counter_memory += 1\n","      state_copy[action] = 0\n","\n","    #print(\"internal_counter_memory\", internal_counter_memory)\n","    return state_copy, reward, internal_counter_memory\n","\n","\n","\n","########################\n","## Get maximum Q_VALUE (not action)\n","#######################\n","def Q_max(state):\n","    Q_values = sess.run(Q, feed_dict={state_holder: state.reshape(1, dim_state)})\n","    return Q_values.max()\n","\n","\n","\n","\n","\n","\n","##############################\n","## ACTUAL RUN\n","###########################\n","\n","time_list = []\n","reward_list = []\n","err_list = []\n","val_list = []\n","mean_reward = 0\n","print_every_n = 50\n","\n","print(\"Here are the\",  trX[:, 0].size ,\" objects and classes to learn\")\n","\n","\n","mean_reward_list = []\n","n_episodes = trX[:, 0].size\n","for k in range(n_episodes):\n","\n","    acc_reward = 0  # Init the accumulated reward\n","    k_n = rd.randint(0, trX[:,0].size)                                            ######   !!!!!!!!!!!!!!!!!\n","\n","    if(epsilon>epsilon_min):\n","        epsilon *= epsi_decay\n","    if (learning_rate > learning_min):\n","        learning_rate *= lr_decay\n","\n","    '''\n","    if k > N_trial+1:\n","        epsilon = 0\n","        learning_rate = 0\n","        observation = teX[k-N_trial, :]\n","        classy = teY[k-N_trial, :]\n","    else:\n","        observation = copy.copy(trX[k, :])\n","        classy = trY[k, :]\n","    '''\n","    observation = copy.copy(trX[k_n, :])\n","    classy = trY[k_n, :]\n","    if(k%50<0):\n","        print(\"Observation:\", observation)\n","\n","\n","    trial_err_list = []\n","    internal_counter_memory = 0\n","    trial_duration = 15\n","    push_finish_button = False\n","\n","    for t in range(trial_duration):  # The number of time steps in this game is maximum 200\n","\n","        old_observation = copy.copy(observation)\n","        old_internal_counter_memory = copy.copy(internal_counter_memory)\n","\n","        Q_values = sess.run(Q, feed_dict={state_holder: observation.reshape(1, dim_state)})\n","        Q_values_reduced = Q_values[0, :]\n","        old_Q_values_reduced = copy.copy(Q_values_reduced)\n","        val = np.max(Q_values_reduced)  # Change in dimension\n","        max_indices = np.where(Q_values_reduced == val)[0]\n","\n","\n","        action = rd.choice(max_indices)\n","        if rd.rand() < epsilon:\n","            action = rd.randint(0, n_action+1)\n","\n","        if(action==n_action):\n","          push_finish_button=True\n","        else:\n","          push_finish_button=False\n","          \n","        reward = 0\n","\n","\n","        new_observation, reward, internal_counter_memory = draw_point(old_observation, action, push_finish_button,internal_counter_memory )\n","\n","        \n","        \n","        if(push_finish_button==False):\n","          reward += time_penalty_reward\n","\n","  \n","        \n","        #print(\"observation: \", observation)\n","        #print(\"new_observation: \", new_observation)\n","        #print(\"action \", action)\n","        #print(\"reward \", reward)\n","        \n","        err = sess.run(error, feed_dict={\n","            state_holder: observation.reshape(1, dim_state),\n","            next_state_holder: new_observation.reshape(1, dim_state),\n","            action_holder: action,\n","            is_done_holder: np.float32(push_finish_button),\n","            r_holder: reward})\n","\n","        sess.run(training_step, feed_dict={\n","            state_holder: observation.reshape(1, dim_state),\n","            next_state_holder: new_observation.reshape(1, dim_state),\n","            action_holder: action,\n","            is_done_holder: np.float32(push_finish_button),\n","            r_holder: reward,\n","            learning_rate_holder: learning_rate})\n","\n","\n","        trial_err_list.append(err)\n","\n","\n","        Q_values = sess.run(Q, feed_dict={state_holder: old_observation.reshape(1, dim_state)})\n","        Q_values_reduced = Q_values[0, :]\n","\n","        observation = new_observation  # Pass the new state to the next step\n","        acc_reward += reward  # Accumulate the reward\n","\n","        weights = sess.run(w_o, feed_dict={state_holder: observation.reshape(1, dim_state)})\n","        \n","\n","        \n","        if(k>=300 and k<305):\n","          if(t==0):\n","            print(\"\\n ================\")\n","            print(\"K= \", k)\n","          print(\"t: \", t)\n","          print(\"Observation: \", observation)\n","          print(\"Action\", action)\n","          print(\"Reward: \", reward)\n","\n","        if(push_finish_button==True):\n","          mean_reward += acc_reward\n","          break\n","          \n","\n","          \n","          \n","        #if((k>=0 and k<5) or (k%100==0) ):\n","        if ((k >= 40 and k < 20)):\n","            if(t==0):\n","                print(\"=======================\")\n","                print(\"==== K= \", k, \"========\")\n","            print(\"t=\", t)\n","            print(\"old_state= \", old_observation)\n","            print(\"old_internal_counter_memory\",old_internal_counter_memory)\n","            print(\"Q_values before: \", old_Q_values_reduced)\n","            print(\"action= \", action)\n","            print(\"Q_values after: \", Q_values_reduced)\n","            print(\"state= \", observation)\n","            print(\"internal_counter_memory\", internal_counter_memory)\n","            print(\"acc_reward= \", acc_reward)\n","            print(\"orig nr. of 1s\", classy)\n","            print(\"w_o= \", weights[0,0])\n","            print(\"------End of trial:\")\n","            print(\"Predicted class: \", internal_counter_memory)\n","            print(\"Real class:      \", classy)\n","            print(\"err1 \", err)\n","            print(\"err2 \", err2)\n","            print(\"---------end of live action -------- \\n \\n\")\n","\n","\n","        \n","\n","        '''\n","        if(k%1000==0 and push_finish_button):\n","        #if (k < 50 and push_finish_button):\n","            print(\"k=\",k)\n","            print(\"mean_reward=\", float(mean_reward)/1000.0)\n","            print(\"Last t= \", internal_counter_memory)\n","            print(\"acc_reward\", acc_reward)\n","            print(\"Nth trial\", k)\n","            print(\"State= \", observation)\n","            print(\"Class= \", classy)\n","            print(\"Action= \", action)\n","            #print(\"w_o= \", weights[0, 0])\n","            #print(\"error\",err)\n","            print(\"==============================\")\n","            mean_reward = 0\n","        '''\n","\n","\n","\n","        #internal_counter_memory += 1\n","\n","        if (push_finish_button==True or t==trial_duration-1):\n","          mean_reward += acc_reward\n","\n","    if (k % print_every_n == 0):\n","        mean_reward = float(mean_reward) / print_every_n\n","        print(\"K\", k)\n","        print(k, \"th mean_reward=\", mean_reward)\n","        print(\"Epsilon: \", epsilon)\n","        mean_reward_list.append(mean_reward)\n","        mean_reward = 0.0\n","        \n","    # Stack values for monitoring\n","\n","    err_list.append(np.mean(trial_err_list))\n","    time_list.append(t + 1)\n","    reward_list.append(acc_reward)  # Store the result\n","    #print(\"acc_reward\", acc_reward)\n","\n","\n","print(\"mean_reward_list\", mean_reward_list)\n","reward_file = 'Mean_rewards/'  + str(n_h_neurons) + '_neurons_' + str(n_action) + '_max_objects'\n","np.save(reward_file, mean_reward_list)\n","\n","\n","matplotlib.get_backend()\n","#print(acc_arr.shape)\n","#print(episodes.shape)\n","#print(\"np.arange(0,n_episodes)\", np.arange(0,n_episodes))\n","legend_string = str(n_h_neurons) + \" neurons/hidden layer\"\n","plt.plot(np.arange(0,len(mean_reward_list)), mean_reward_list, label = legend_string)\n","plt.legend()\n","print(\"legend_string= \", legend_string)\n","#plt.legend(legend_string,)\n","plt.ylabel('Success rate')\n","plt.xlabel('Episodes')\n","plt.ylim(0,1)\n","title_string = 'Success rate for agent when counting binaries between 0 and ' + str(n_action) + ' binaries'\n","plt.title(title_string)\n","plt.savefig('Q_learning_binaries.png')\n","plt.show()\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["(1000, 1)\n","(1000, 1)\n","(1000, 1)\n","(1000, 1)\n","('Here are the', 1000, ' objects and classes to learn')\n","('K', 0)\n","(0, 'th mean_reward=', -0.004)\n","('Epsilon: ', 0.99)\n","('K', 50)\n","(50, 'th mean_reward=', -0.14400000000000004)\n","('Epsilon: ', 0.5989560064661611)\n","('K', 100)\n","(100, 'th mean_reward=', -0.096)\n","('Epsilon: ', 0.36237201786049694)\n","('K', 150)\n","(150, 'th mean_reward=', -0.2)\n","('Epsilon: ', 0.21923726936647234)\n","('K', 200)\n","(200, 'th mean_reward=', -0.10000000000000002)\n","('Epsilon: ', 0.13263987810938213)\n","('K', 250)\n","(250, 'th mean_reward=', -0.12)\n","('Epsilon: ', 0.08024793100055952)\n","\n"," ================\n","('K= ', 300)\n","('t: ', 0)\n","('Observation: ', array([255.]))\n","('Action', 1)\n","('Reward: ', 0)\n","('K', 300)\n","(300, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.0485504851305729)\n","\n"," ================\n","('K= ', 301)\n","('t: ', 0)\n","('Observation: ', array([255.]))\n","('Action', 1)\n","('Reward: ', 0)\n","\n"," ================\n","('K= ', 302)\n","('t: ', 0)\n","('Observation: ', array([255.]))\n","('Action', 1)\n","('Reward: ', 0)\n","\n"," ================\n","('K= ', 303)\n","('t: ', 0)\n","('Observation: ', array([255.]))\n","('Action', 1)\n","('Reward: ', 0)\n","\n"," ================\n","('K= ', 304)\n","('t: ', 0)\n","('Observation: ', array([255.]))\n","('Action', 1)\n","('Reward: ', 0)\n","('K', 350)\n","(350, 'th mean_reward=', -0.06000000000000001)\n","('Epsilon: ', 0.029373338066467324)\n","('K', 400)\n","(400, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.017771047742294682)\n","('K', 450)\n","(450, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.010751591703479106)\n","('K', 500)\n","(500, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.009920974201040588)\n","('K', 550)\n","(550, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.009920974201040588)\n","('K', 600)\n","(600, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.009920974201040588)\n","('K', 650)\n","(650, 'th mean_reward=', -0.06000000000000001)\n","('Epsilon: ', 0.009920974201040588)\n","('K', 700)\n","(700, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.009920974201040588)\n","('K', 750)\n","(750, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.009920974201040588)\n","('K', 800)\n","(800, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.009920974201040588)\n","('K', 850)\n","(850, 'th mean_reward=', -0.06000000000000001)\n","('Epsilon: ', 0.009920974201040588)\n","('K', 900)\n","(900, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.009920974201040588)\n","('K', 950)\n","(950, 'th mean_reward=', 0.0)\n","('Epsilon: ', 0.009920974201040588)\n","('mean_reward_list', [-0.004, -0.14400000000000004, -0.096, -0.2, -0.10000000000000002, -0.12, 0.0, -0.06000000000000001, 0.0, 0.0, 0.0, 0.0, 0.0, -0.06000000000000001, 0.0, 0.0, 0.0, -0.06000000000000001, 0.0, 0.0])\n"],"name":"stdout"},{"output_type":"error","ename":"IOError","evalue":"ignored","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-18-ceca1b51c8f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean_reward_list\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0mreward_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Mean_rewards/'\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_h_neurons\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_neurons_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_max_objects'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_reward_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36msave\u001b[0;34m(file, arr, allow_pickle, fix_imports)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.npy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'Mean_rewards/600_neurons_1_max_objects.npy'"]}]},{"metadata":{"id":"G40o5CrdOngY","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}